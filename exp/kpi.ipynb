{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bab678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test.csv æ—¶é—´åºåˆ—é•¿åº¦åˆ†æå·¥å…·\n",
      "============================================================\n",
      "æ—¶é—´åºåˆ—é•¿åº¦åˆ†æ (Time Series Length Analysis)\n",
      "============================================================\n",
      "æ–‡ä»¶: /home/smore/spectrum/KPI-Anomaly-Detection/Finals_dataset/test.csv\n",
      "æ–‡ä»¶å¤§å°: 175.25 MB\n",
      "\n",
      "åŠ è½½æ•°æ®ä¸­...\n",
      "æ€»æ•°æ®ç‚¹: 3,072,928\n",
      "label\n",
      "0    3018368\n",
      "1      54560\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "æ¯ä¸ªKPIçš„æ—¶é—´åºåˆ—é•¿åº¦ç»Ÿè®¡\n",
      "============================================================\n",
      "å”¯ä¸€KPIæ•°é‡: 29\n",
      "\n",
      "è¯¦ç»†ç»Ÿè®¡:\n",
      "  æœ€çŸ­åºåˆ—é•¿åº¦: 8,768\n",
      "  æœ€é•¿åºåˆ—é•¿åº¦: 149,504\n",
      "  å¹³å‡åºåˆ—é•¿åº¦: 105963.03\n",
      "  ä¸­ä½æ•°åºåˆ—é•¿åº¦: 131776.00\n",
      "  æ ‡å‡†å·®: 56996.25\n",
      "\n",
      "åºåˆ—é•¿åº¦åˆ†å¸ƒ:\n",
      "  é•¿åº¦ 8,768: 4 ä¸ªKPI (13.79%)\n",
      "  é•¿åº¦ 8,896: 1 ä¸ªKPI (3.45%)\n",
      "  é•¿åº¦ 8,928: 1 ä¸ªKPI (3.45%)\n",
      "  é•¿åº¦ 12,448: 1 ä¸ªKPI (3.45%)\n",
      "  é•¿åº¦ 108,000: 1 ä¸ªKPI (3.45%)\n",
      "  é•¿åº¦ 110,656: 1 ä¸ªKPI (3.45%)\n",
      "  é•¿åº¦ 110,688: 1 ä¸ªKPI (3.45%)\n",
      "  é•¿åº¦ 111,360: 1 ä¸ªKPI (3.45%)\n",
      "  é•¿åº¦ 131,744: 2 ä¸ªKPI (6.90%)\n",
      "  é•¿åº¦ 131,776: 5 ä¸ªKPI (17.24%)\n",
      "  é•¿åº¦ 149,472: 1 ä¸ªKPI (3.45%)\n",
      "  é•¿åº¦ 149,504: 10 ä¸ªKPI (34.48%)\n",
      "\n",
      "============================================================\n",
      "æ¯ä¸ªKPIçš„å…·ä½“é•¿åº¦\n",
      "============================================================\n",
      "KPI ID                                   é•¿åº¦         ç™¾åˆ†æ¯”       \n",
      "------------------------------------------------------------\n",
      "05f10d3a-239c-3bef-9bdc-a2feeb0037aa     149,504    4.8652    %\n",
      "0efb375b-b902-3661-ab23-9a0bb799f4e3     8,768      0.2853    %\n",
      "1c6d7a26-1f1a-3321-bb4d-7a9d969ec8f0     149,504    4.8652    %\n",
      "301c70d8-1630-35ac-8f96-bc1b6f4359ea     8,768      0.2853    %\n",
      "42d6616d-c9c5-370a-a8ba-17ead74f3114     149,504    4.8652    %\n",
      "43115f2a-baeb-3b01-96f7-4ea14188343c     110,688    3.6020    %\n",
      "431a8542-c468-3988-a508-3afd06a218da     131,776    4.2883    %\n",
      "4d2af31a-9916-3d9f-8a8e-8a268a48c095     131,776    4.2883    %\n",
      "54350a12-7a9d-3ca8-b81f-f886b9d156fd     8,928      0.2905    %\n",
      "55f8b8b8-b659-38df-b3df-e4a5a8a54bc9     149,504    4.8652    %\n",
      "57051487-3a40-3828-9084-a12f7f23ee38     131,776    4.2883    %\n",
      "6a757df4-95e5-3357-8406-165e2bd49360     131,744    4.2872    %\n",
      "6d1114ae-be04-3c46-b5aa-be1a003a57cd     149,504    4.8652    %\n",
      "6efa3a07-4544-34a0-b921-a155bd1a05e8     149,472    4.8642    %\n",
      "7103fa0f-cac4-314f-addc-866190247439     131,776    4.2883    %\n",
      "847e8ecc-f8d2-3a93-9107-f367a0aab37d     149,504    4.8652    %\n",
      "8723f0fb-eaef-32e6-b372-6034c9c04b80     149,504    4.8652    %\n",
      "9c639a46-34c8-39bc-aaf0-9144b37adfc8     110,656    3.6010    %\n",
      "a07ac296-de40-3a7c-8df3-91f642cc14d0     111,360    3.6239    %\n",
      "a8c06b47-cc41-3738-9110-12df0ee4c721     8,896      0.2895    %\n",
      "ab216663-dcc2-3a24-b1ee-2c3e550e06c9     12,448     0.4051    %\n",
      "adb2fde9-8589-3f5b-a410-5fe14386c7af     149,504    4.8652    %\n",
      "ba5f3328-9f3f-3ff5-a683-84437d16d554     149,504    4.8652    %\n",
      "c02607e8-7399-3dde-9d28-8a8da5e5d251     8,768      0.2853    %\n",
      "c69a50cf-ee03-3bd7-831e-407d36c7ee91     149,504    4.8652    %\n",
      "da10a69f-d836-3baa-ad40-3e548ecf1fbd     108,000    3.5146    %\n",
      "e0747cad-8dc8-38a9-a9ab-855b61f5551d     8,768      0.2853    %\n",
      "f0932edd-6400-3e63-9559-0a9860a1baa9     131,744    4.2872    %\n",
      "ffb82d38-5f00-37db-abc0-5d2e4e4cb6aa     131,776    4.2883    %\n",
      "\n",
      "============================================================\n",
      "æ—¶é—´èŒƒå›´åˆ†æ\n",
      "============================================================\n",
      "æ€»ä½“æ—¶é—´èŒƒå›´:\n",
      "  å¼€å§‹æ—¶é—´: 2016-08-12 23:35:00\n",
      "  ç»“æŸæ—¶é—´: 2017-10-30 16:55:00\n",
      "  æ€»æŒç»­æ—¶é—´: 443 days 17:20:00\n",
      "\n",
      "æ¯ä¸ªKPIçš„æ—¶é—´èŒƒå›´:\n",
      "KPI ID                                   å¼€å§‹æ—¶é—´                 ç»“æŸæ—¶é—´                 æŒç»­æ—¶é—´           \n",
      "----------------------------------------------------------------------------------------------------\n",
      "05f10d3a-239c-3bef-9bdc-a2feeb0037aa     2017-07-14 06:25     2017-10-26 02:08     103 days 19:43:00\n",
      "0efb375b-b902-3661-ab23-9a0bb799f4e3     2016-10-04 05:20     2016-11-03 15:55     30 days 10:35:00\n",
      "1c6d7a26-1f1a-3321-bb4d-7a9d969ec8f0     2017-07-14 06:15     2017-10-26 01:58     103 days 19:43:00\n",
      "301c70d8-1630-35ac-8f96-bc1b6f4359ea     2016-10-24 04:00     2016-11-23 14:35     30 days 10:35:00\n",
      "42d6616d-c9c5-370a-a8ba-17ead74f3114     2017-07-14 06:15     2017-10-26 01:58     103 days 19:43:00\n",
      "43115f2a-baeb-3b01-96f7-4ea14188343c     2017-08-10 05:30     2017-10-26 02:17     76 days 20:47:00\n",
      "431a8542-c468-3988-a508-3afd06a218da     2017-07-31 04:40     2017-10-30 16:55     91 days 12:15:00\n",
      "4d2af31a-9916-3d9f-8a8e-8a268a48c095     2017-07-31 04:35     2017-10-30 16:50     91 days 12:15:00\n",
      "54350a12-7a9d-3ca8-b81f-f886b9d156fd     2016-08-22 16:00     2016-09-22 15:55     30 days 23:55:00\n",
      "55f8b8b8-b659-38df-b3df-e4a5a8a54bc9     2017-07-14 06:15     2017-10-26 01:58     103 days 19:43:00\n",
      "... (è¿˜æœ‰ 19 ä¸ªKPI)\n",
      "\n",
      "============================================================\n",
      "æ¯ä¸ªKPIçš„å¼‚å¸¸ç‚¹ç»Ÿè®¡\n",
      "============================================================\n",
      "KPI ID                                   æ€»ç‚¹æ•°        å¼‚å¸¸ç‚¹æ•°       å¼‚å¸¸ç‡       \n",
      "---------------------------------------------------------------------------\n",
      "da10a69f-d836-3baa-ad40-3e548ecf1fbd     108,000    8,750      8.1019    %\n",
      "ba5f3328-9f3f-3ff5-a683-84437d16d554     149,504    6,777      4.5330    %\n",
      "55f8b8b8-b659-38df-b3df-e4a5a8a54bc9     149,504    6,091      4.0741    %\n",
      "6efa3a07-4544-34a0-b921-a155bd1a05e8     149,472    5,283      3.5344    %\n",
      "431a8542-c468-3988-a508-3afd06a218da     131,776    3,286      2.4936    %\n",
      "301c70d8-1630-35ac-8f96-bc1b6f4359ea     8,768      206        2.3495    %\n",
      "ffb82d38-5f00-37db-abc0-5d2e4e4cb6aa     131,776    2,855      2.1666    %\n",
      "f0932edd-6400-3e63-9559-0a9860a1baa9     131,744    2,842      2.1572    %\n",
      "a07ac296-de40-3a7c-8df3-91f642cc14d0     111,360    2,259      2.0286    %\n",
      "6a757df4-95e5-3357-8406-165e2bd49360     131,744    2,638      2.0024    %\n",
      "4d2af31a-9916-3d9f-8a8e-8a268a48c095     131,776    2,585      1.9617    %\n",
      "ab216663-dcc2-3a24-b1ee-2c3e550e06c9     12,448     187        1.5022    %\n",
      "a8c06b47-cc41-3738-9110-12df0ee4c721     8,896      127        1.4276    %\n",
      "e0747cad-8dc8-38a9-a9ab-855b61f5551d     8,768      116        1.3230    %\n",
      "42d6616d-c9c5-370a-a8ba-17ead74f3114     149,504    1,896      1.2682    %\n",
      "... (è¿˜æœ‰ 14 ä¸ªKPI)\n",
      "\n",
      "å¼‚å¸¸ç‡ç»Ÿè®¡:\n",
      "  å¹³å‡å¼‚å¸¸ç‡: 1.7150%\n",
      "  æœ€é«˜å¼‚å¸¸ç‡: 8.1019%\n",
      "  æœ€ä½å¼‚å¸¸ç‡: 0.0349%\n",
      "  æœ‰å¼‚å¸¸çš„KPIæ•°é‡: 29\n",
      "  æ— å¼‚å¸¸çš„KPIæ•°é‡: 0\n",
      "\n",
      "============================================================\n",
      "æ€»ç»“æŠ¥å‘Š (SUMMARY REPORT)\n",
      "============================================================\n",
      "ğŸ“Š æ•°æ®æ¦‚è§ˆ:\n",
      "   â€¢ æ€»KPIæ•°é‡: 29\n",
      "   â€¢ æ€»æ•°æ®ç‚¹: 3,072,928\n",
      "   â€¢ å¹³å‡æ¯ä¸ªKPIé•¿åº¦: 105963 ä¸ªæ•°æ®ç‚¹\n",
      "\n",
      "ğŸ“ åºåˆ—é•¿åº¦ç»Ÿè®¡:\n",
      "   â€¢ æœ€çŸ­åºåˆ—: 8,768 ä¸ªæ•°æ®ç‚¹\n",
      "   â€¢ æœ€é•¿åºåˆ—: 149,504 ä¸ªæ•°æ®ç‚¹\n",
      "   â€¢ ä¸­ä½æ•°é•¿åº¦: 131776 ä¸ªæ•°æ®ç‚¹\n",
      "\n",
      "âš ï¸  æ—¶é—´åºåˆ—é•¿åº¦ä¸ä¸€è‡´\n",
      "   â€¢ é•¿åº¦å·®å¼‚: 140,736 ä¸ªæ•°æ®ç‚¹\n",
      "\n",
      "============================================================\n",
      "åˆ†æå®Œæˆ âœ…\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def analyze_time_series_lengths():\n",
    "    \"\"\"Analyze time series lengths for each KPI in test.csv\"\"\"\n",
    "\n",
    "    test_file = \"/home/smore/spectrum/KPI-Anomaly-Detection/Finals_dataset/test.csv\"\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"Error: Test file not found: {test_file}\")\n",
    "        return None\n",
    "\n",
    "    print(\"æ—¶é—´åºåˆ—é•¿åº¦åˆ†æ (Time Series Length Analysis)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"æ–‡ä»¶: {test_file}\")\n",
    "    print(f\"æ–‡ä»¶å¤§å°: {os.path.getsize(test_file) / (1024*1024):.2f} MB\")\n",
    "\n",
    "    try:\n",
    "        print(\"\\nåŠ è½½æ•°æ®ä¸­...\")\n",
    "        df = pd.read_csv(test_file)\n",
    "\n",
    "        print(f\"æ€»æ•°æ®ç‚¹: {len(df):,}\")\n",
    "\n",
    "        # Check if required columns exist\n",
    "        if \"KPI ID\" not in df.columns:\n",
    "            print(\"é”™è¯¯: æœªæ‰¾åˆ° 'KPI ID' åˆ—\")\n",
    "            return None\n",
    "        print(df[\"label\"].value_counts())\n",
    "        # Calculate time series length for each KPI\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"æ¯ä¸ªKPIçš„æ—¶é—´åºåˆ—é•¿åº¦ç»Ÿè®¡\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        kpi_lengths = df[\"KPI ID\"].value_counts().sort_index()\n",
    "\n",
    "        print(f\"å”¯ä¸€KPIæ•°é‡: {len(kpi_lengths)}\")\n",
    "\n",
    "        # Display detailed statistics\n",
    "        print(f\"\\nè¯¦ç»†ç»Ÿè®¡:\")\n",
    "        print(f\"  æœ€çŸ­åºåˆ—é•¿åº¦: {kpi_lengths.min():,}\")\n",
    "        print(f\"  æœ€é•¿åºåˆ—é•¿åº¦: {kpi_lengths.max():,}\")\n",
    "        print(f\"  å¹³å‡åºåˆ—é•¿åº¦: {kpi_lengths.mean():.2f}\")\n",
    "        print(f\"  ä¸­ä½æ•°åºåˆ—é•¿åº¦: {kpi_lengths.median():.2f}\")\n",
    "        print(f\"  æ ‡å‡†å·®: {kpi_lengths.std():.2f}\")\n",
    "\n",
    "        # Show length distribution\n",
    "        print(f\"\\nåºåˆ—é•¿åº¦åˆ†å¸ƒ:\")\n",
    "        length_distribution = kpi_lengths.value_counts().sort_index()\n",
    "        for length, count in length_distribution.items():\n",
    "            percentage = (count / len(kpi_lengths)) * 100\n",
    "            print(f\"  é•¿åº¦ {length:,}: {count} ä¸ªKPI ({percentage:.2f}%)\")\n",
    "\n",
    "        # Display each KPI's length\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"æ¯ä¸ªKPIçš„å…·ä½“é•¿åº¦\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        print(f\"{'KPI ID':<40} {'é•¿åº¦':<10} {'ç™¾åˆ†æ¯”':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for kpi_id, length in kpi_lengths.items():\n",
    "            percentage = (length / len(df)) * 100\n",
    "            print(f\"{str(kpi_id):<40} {length:<10,} {percentage:<10.4f}%\")\n",
    "\n",
    "        # Time analysis if timestamp column exists\n",
    "        if \"timestamp\" in df.columns:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"æ—¶é—´èŒƒå›´åˆ†æ\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            df[\"datetime\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\")\n",
    "\n",
    "            print(f\"æ€»ä½“æ—¶é—´èŒƒå›´:\")\n",
    "            print(f\"  å¼€å§‹æ—¶é—´: {df['datetime'].min()}\")\n",
    "            print(f\"  ç»“æŸæ—¶é—´: {df['datetime'].max()}\")\n",
    "            print(f\"  æ€»æŒç»­æ—¶é—´: {df['datetime'].max() - df['datetime'].min()}\")\n",
    "\n",
    "            # Analyze time range for each KPI\n",
    "            print(f\"\\næ¯ä¸ªKPIçš„æ—¶é—´èŒƒå›´:\")\n",
    "            print(f\"{'KPI ID':<40} {'å¼€å§‹æ—¶é—´':<20} {'ç»“æŸæ—¶é—´':<20} {'æŒç»­æ—¶é—´':<15}\")\n",
    "            print(\"-\" * 100)\n",
    "\n",
    "            for kpi_id in kpi_lengths.index[:10]:  # Show first 10 KPIs\n",
    "                kpi_data = df[df[\"KPI ID\"] == kpi_id]\n",
    "                start_time = kpi_data[\"datetime\"].min()\n",
    "                end_time = kpi_data[\"datetime\"].max()\n",
    "                duration = end_time - start_time\n",
    "\n",
    "                print(\n",
    "                    f\"{str(kpi_id):<40} {start_time.strftime('%Y-%m-%d %H:%M'):<20} {end_time.strftime('%Y-%m-%d %H:%M'):<20} {str(duration):<15}\"\n",
    "                )\n",
    "\n",
    "            if len(kpi_lengths) > 10:\n",
    "                print(f\"... (è¿˜æœ‰ {len(kpi_lengths) - 10} ä¸ªKPI)\")\n",
    "\n",
    "        # Anomaly analysis if label column exists\n",
    "        if \"label\" in df.columns:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"æ¯ä¸ªKPIçš„å¼‚å¸¸ç‚¹ç»Ÿè®¡\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            kpi_anomalies = (\n",
    "                df.groupby(\"KPI ID\")[\"label\"].agg([\"count\", \"sum\", \"mean\"]).round(6)\n",
    "            )\n",
    "            kpi_anomalies.columns = [\"æ€»ç‚¹æ•°\", \"å¼‚å¸¸ç‚¹æ•°\", \"å¼‚å¸¸ç‡\"]\n",
    "            kpi_anomalies = kpi_anomalies.sort_values(\"å¼‚å¸¸ç‡\", ascending=False)\n",
    "\n",
    "            print(f\"{'KPI ID':<40} {'æ€»ç‚¹æ•°':<10} {'å¼‚å¸¸ç‚¹æ•°':<10} {'å¼‚å¸¸ç‡':<10}\")\n",
    "            print(\"-\" * 75)\n",
    "\n",
    "            for kpi_id, row in kpi_anomalies.head(15).iterrows():\n",
    "                print(\n",
    "                    f\"{str(kpi_id):<40} {int(row['æ€»ç‚¹æ•°']):<10,} {int(row['å¼‚å¸¸ç‚¹æ•°']):<10,} {row['å¼‚å¸¸ç‡']*100:<10.4f}%\"\n",
    "                )\n",
    "\n",
    "            if len(kpi_anomalies) > 15:\n",
    "                print(f\"... (è¿˜æœ‰ {len(kpi_anomalies) - 15} ä¸ªKPI)\")\n",
    "\n",
    "            # Summary statistics\n",
    "            print(f\"\\nå¼‚å¸¸ç‡ç»Ÿè®¡:\")\n",
    "            print(f\"  å¹³å‡å¼‚å¸¸ç‡: {kpi_anomalies['å¼‚å¸¸ç‡'].mean()*100:.4f}%\")\n",
    "            print(f\"  æœ€é«˜å¼‚å¸¸ç‡: {kpi_anomalies['å¼‚å¸¸ç‡'].max()*100:.4f}%\")\n",
    "            print(f\"  æœ€ä½å¼‚å¸¸ç‡: {kpi_anomalies['å¼‚å¸¸ç‡'].min()*100:.4f}%\")\n",
    "            print(f\"  æœ‰å¼‚å¸¸çš„KPIæ•°é‡: {(kpi_anomalies['å¼‚å¸¸ç‚¹æ•°'] > 0).sum()}\")\n",
    "            print(f\"  æ— å¼‚å¸¸çš„KPIæ•°é‡: {(kpi_anomalies['å¼‚å¸¸ç‚¹æ•°'] == 0).sum()}\")\n",
    "\n",
    "        return {\n",
    "            \"kpi_lengths\": kpi_lengths,\n",
    "            \"total_kpis\": len(kpi_lengths),\n",
    "            \"total_points\": len(df),\n",
    "            \"min_length\": kpi_lengths.min(),\n",
    "            \"max_length\": kpi_lengths.max(),\n",
    "            \"mean_length\": kpi_lengths.mean(),\n",
    "            \"median_length\": kpi_lengths.median(),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_summary_report(results):\n",
    "    \"\"\"Create a summary report\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"æ€»ç»“æŠ¥å‘Š (SUMMARY REPORT)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(f\"ğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "    print(f\"   â€¢ æ€»KPIæ•°é‡: {results['total_kpis']}\")\n",
    "    print(f\"   â€¢ æ€»æ•°æ®ç‚¹: {results['total_points']:,}\")\n",
    "    print(f\"   â€¢ å¹³å‡æ¯ä¸ªKPIé•¿åº¦: {results['mean_length']:.0f} ä¸ªæ•°æ®ç‚¹\")\n",
    "\n",
    "    print(f\"\\nğŸ“ åºåˆ—é•¿åº¦ç»Ÿè®¡:\")\n",
    "    print(f\"   â€¢ æœ€çŸ­åºåˆ—: {results['min_length']:,} ä¸ªæ•°æ®ç‚¹\")\n",
    "    print(f\"   â€¢ æœ€é•¿åºåˆ—: {results['max_length']:,} ä¸ªæ•°æ®ç‚¹\")\n",
    "    print(f\"   â€¢ ä¸­ä½æ•°é•¿åº¦: {results['median_length']:.0f} ä¸ªæ•°æ®ç‚¹\")\n",
    "\n",
    "    # Check if all series have the same length\n",
    "    if results[\"min_length\"] == results[\"max_length\"]:\n",
    "        print(f\"\\nâœ… æ‰€æœ‰æ—¶é—´åºåˆ—é•¿åº¦ç›¸åŒ: {results['min_length']:,} ä¸ªæ•°æ®ç‚¹\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  æ—¶é—´åºåˆ—é•¿åº¦ä¸ä¸€è‡´\")\n",
    "        print(\n",
    "            f\"   â€¢ é•¿åº¦å·®å¼‚: {results['max_length'] - results['min_length']:,} ä¸ªæ•°æ®ç‚¹\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Test.csv æ—¶é—´åºåˆ—é•¿åº¦åˆ†æå·¥å…·\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = analyze_time_series_lengths()\n",
    "\n",
    "if results:\n",
    "    create_summary_report(results)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"åˆ†æå®Œæˆ âœ…\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"åˆ†æå¤±è´¥ âŒ\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC Implementation\n",
    "# Based on Self-adversarial VAE with spectral residual and Donut's seasonal KPI methods\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter1d\n",
    "from typing import Tuple, Optional, Dict, Any\n",
    "\n",
    "\n",
    "class MCMCImputer:\n",
    "    \"\"\"\n",
    "    MCMC time series imputer\n",
    "\n",
    "    Features:\n",
    "    - Spectral residual anomaly detection (Self-adversarial VAE)\n",
    "    - Seasonal pattern modeling (Donut method)\n",
    "    - Adaptive MCMC sampling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_iter: int = 500,\n",
    "        burn_in: int = 100,\n",
    "        use_spectral: bool = True,\n",
    "        adaptive: bool = True,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        self.n_iter = n_iter\n",
    "        self.burn_in = burn_in\n",
    "        self.use_spectral = use_spectral\n",
    "        self.adaptive = adaptive\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def _detect_missing_points(self, values: pl.Series) -> pl.Series:\n",
    "        \"\"\"Detect missing timestamps\"\"\"\n",
    "\n",
    "        if len(values) < 2:\n",
    "            return pl.Series(\"is_missing\", [False] * len(values))\n",
    "\n",
    "        return values.is_null()\n",
    "\n",
    "    def _detect_seasonality(self, values: pl.Series) -> Tuple[int, float]:\n",
    "        \"\"\"FFT-based seasonality detection\"\"\"\n",
    "\n",
    "        if len(values) < 50:\n",
    "            return (min(24, len(values) // 4), 0.1)\n",
    "\n",
    "        # Detrend\n",
    "        detrended = values - np.linspace(values[0], values[-1], len(values))\n",
    "\n",
    "        # FFT\n",
    "        fft_vals = np.abs(np.fft.fft(detrended))\n",
    "        freqs = np.fft.fftfreq(len(detrended))\n",
    "\n",
    "        # Find peaks\n",
    "        peaks, _ = find_peaks(\n",
    "            fft_vals[: len(fft_vals) // 2], height=np.max(fft_vals) * 0.1\n",
    "        )\n",
    "\n",
    "        if len(peaks) == 0:\n",
    "            return (min(24, len(values) // 4), 0.1)\n",
    "\n",
    "        # Get strongest peak\n",
    "        peak_strengths = fft_vals[peaks]\n",
    "        strongest_idx = peaks[np.argmax(peak_strengths)]\n",
    "\n",
    "        if freqs[strongest_idx] > 0:\n",
    "            period = int(1 / freqs[strongest_idx])\n",
    "            period = max(4, min(period, len(values) // 4))\n",
    "        else:\n",
    "            period = min(24, len(values) // 4)\n",
    "\n",
    "        strength = peak_strengths[np.argmax(peak_strengths)] / np.max(fft_vals)\n",
    "        strength = min(strength, 0.8)\n",
    "\n",
    "        return (period, strength)\n",
    "\n",
    "    def _spectral_residual_scores(self, values_np):\n",
    "        \"\"\"Spectral residual anomaly detection\"\"\"\n",
    "\n",
    "        if len(values_np) < 10:\n",
    "            return np.zeros(len(values_np))\n",
    "\n",
    "        # FFT\n",
    "        fft_vals = np.fft.fft(values_np)\n",
    "        magnitude = np.abs(fft_vals)\n",
    "        phase = np.angle(fft_vals)\n",
    "\n",
    "        # Log magnitude\n",
    "        log_mag = np.log(magnitude + 1e-8)\n",
    "\n",
    "        # Smooth\n",
    "        kernel_size = max(3, len(values_np) // 20)\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "        padded = np.pad(log_mag, padding, mode=\"reflect\")\n",
    "        smoothed = np.convolve(padded, np.ones(kernel_size) / kernel_size, mode=\"valid\")\n",
    "\n",
    "        # Spectral residual\n",
    "        residual = log_mag - smoothed\n",
    "\n",
    "        # Reconstruct\n",
    "        reconstructed_fft = np.exp(residual + 1j * phase)\n",
    "        reconstructed = np.abs(np.fft.ifft(reconstructed_fft))\n",
    "\n",
    "        # Smooth and normalize\n",
    "        scores = gaussian_filter1d(reconstructed, sigma=1.0)\n",
    "        scores = scores - np.min(scores)\n",
    "\n",
    "        if np.max(scores) > 0:\n",
    "            scores = scores / np.max(scores)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def _compute_trend(self, values: pl.Series):\n",
    "        \"\"\"Compute trend component\"\"\"\n",
    "        window_size = max(3, min(len(values) // 4, 20))\n",
    "        if window_size % 2 == 0:\n",
    "            window_size += 1\n",
    "        return uniform_filter1d(values, size=window_size, mode=\"nearest\")\n",
    "\n",
    "    def _compute_seasonal(self, values_np, period, strength):\n",
    "        \"\"\"Compute seasonal component\"\"\"\n",
    "        n = len(values_np)\n",
    "        seasonal = np.zeros(n)\n",
    "\n",
    "        if period <= 1 or period >= n:\n",
    "            return seasonal\n",
    "\n",
    "        for i in range(period):\n",
    "            indices = np.arange(i, n, period)\n",
    "            if len(indices) > 1:\n",
    "                seasonal_value = np.median(values_np[indices])\n",
    "                seasonal[indices] = seasonal_value\n",
    "\n",
    "        seasonal = seasonal - np.mean(seasonal)\n",
    "        seasonal = seasonal * strength\n",
    "\n",
    "        return seasonal\n",
    "\n",
    "    def _mcmc_step(\n",
    "        self,\n",
    "        values_np,\n",
    "        missing_mask_np,\n",
    "        trend_np,\n",
    "        seasonal_np,\n",
    "        anomaly_scores_np,\n",
    "        iter_num,\n",
    "    ):\n",
    "        \"\"\"Single MCMC Gibbs sampling step\"\"\"\n",
    "\n",
    "        result = values_np.copy()\n",
    "        missing_indices = np.where(missing_mask_np)[0]\n",
    "\n",
    "        # Adaptive variance\n",
    "        if self.adaptive:\n",
    "            exploration = max(0.1, 1.0 - iter_num / self.n_iter)\n",
    "            base_var = 0.01 * exploration\n",
    "        else:\n",
    "            base_var = 0.01\n",
    "\n",
    "        for idx in missing_indices:\n",
    "            # Prior\n",
    "            prior_mean = trend_np[idx] + seasonal_np[idx]\n",
    "\n",
    "            # Anomaly adjustment\n",
    "            anomaly_weight = 1.0\n",
    "            if self.use_spectral and idx < len(anomaly_scores_np):\n",
    "                anomaly_weight = 1.0 + anomaly_scores_np[idx] * 2.0\n",
    "\n",
    "            # Neighbors\n",
    "            neighbors = []\n",
    "            weights = []\n",
    "\n",
    "            search_radius = min(10, len(values_np) // 20)\n",
    "            for offset in range(-search_radius, search_radius + 1):\n",
    "                if offset == 0:\n",
    "                    continue\n",
    "\n",
    "                neighbor_idx = idx + offset\n",
    "                if (\n",
    "                    0 <= neighbor_idx < len(values_np)\n",
    "                    and not missing_mask_np[neighbor_idx]\n",
    "                ):\n",
    "                    neighbors.append(values_np[neighbor_idx])\n",
    "                    weight = 1.0 / (abs(offset) + 1)\n",
    "                    if self.use_spectral and neighbor_idx < len(anomaly_scores_np):\n",
    "                        weight = weight / (anomaly_scores_np[neighbor_idx] + 0.1)\n",
    "                    weights.append(weight)\n",
    "\n",
    "            if neighbors:\n",
    "                neighbors = np.array(neighbors)\n",
    "                weights = np.array(weights)\n",
    "                weights = weights / np.sum(weights)\n",
    "\n",
    "                likelihood_mean = np.sum(neighbors * weights)\n",
    "\n",
    "                # Bayesian update\n",
    "                prior_precision = 1.0 / (base_var * anomaly_weight)\n",
    "                likelihood_precision = len(neighbors) / (base_var * anomaly_weight)\n",
    "\n",
    "                posterior_precision = prior_precision + likelihood_precision\n",
    "                posterior_mean = (\n",
    "                    prior_precision * prior_mean\n",
    "                    + likelihood_precision * likelihood_mean\n",
    "                ) / posterior_precision\n",
    "                posterior_var = 1.0 / posterior_precision\n",
    "            else:\n",
    "                posterior_mean = prior_mean\n",
    "                posterior_var = base_var * anomaly_weight\n",
    "\n",
    "            # Sample\n",
    "            result[idx] = np.random.normal(posterior_mean, np.sqrt(posterior_var))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def fit_transform(self, timestamps: pl.Series, values: pl.Series) -> pl.Series:\n",
    "        \"\"\"\n",
    "        Main imputation method\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"Starting MCMC imputation (n_iter={self.n_iter})...\")\n",
    "\n",
    "        # Step 1: Detect missing points\n",
    "        missing_mask = self._detect_missing_points(values)\n",
    "\n",
    "        n_missing = missing_mask.sum()\n",
    "        if n_missing == 0:\n",
    "            print(\"No missing values detected\")\n",
    "            return values\n",
    "\n",
    "        print(f\"   Detected: {n_missing} missing values\")\n",
    "\n",
    "        # Step 2: Initialize missing values\n",
    "        current_values = values\n",
    "        observed_mask = ~missing_mask\n",
    "\n",
    "        missing_indices = np.where(missing_mask)[0]\n",
    "        for idx in missing_indices:\n",
    "            left = current_values[:idx][observed_mask[:idx]]\n",
    "            right = current_values[idx + 1 :][observed_mask[idx + 1 :]]\n",
    "\n",
    "            if len(left) > 0 and len(right) > 0:\n",
    "                current_values[idx] = (left[-1] + right[0]) / 2\n",
    "            elif len(left) > 0:\n",
    "                current_values[idx] = left[-1]\n",
    "            elif len(right) > 0:\n",
    "                current_values[idx] = right[0]\n",
    "            else:\n",
    "                current_values[idx] = np.nanmean(current_values[observed_mask])\n",
    "\n",
    "        # Step 3: Seasonality detection\n",
    "        observed_values = current_values[observed_mask]\n",
    "\n",
    "        seasonal_period, seasonal_strength = self._detect_seasonality(observed_values)\n",
    "\n",
    "        print(\n",
    "            f\"   Seasonality: period={seasonal_period}, strength={seasonal_strength:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Step 4: Anomaly scores\n",
    "        anomaly_scores = np.zeros(len(current_values))\n",
    "        if self.use_spectral and len(observed_values) > 20:\n",
    "            anomaly_scores[observed_mask] = self._spectral_residual_scores(\n",
    "                observed_values\n",
    "            )\n",
    "            for idx in missing_indices:\n",
    "                if idx > 0 and idx < len(anomaly_scores) - 1:\n",
    "                    anomaly_scores[idx] = (\n",
    "                        anomaly_scores[idx - 1] + anomaly_scores[idx + 1]\n",
    "                    ) / 2\n",
    "\n",
    "        # Step 5: MCMC sampling\n",
    "        samples = []\n",
    "\n",
    "        for iteration in range(self.n_iter):\n",
    "            # Compute components\n",
    "            trend = self._compute_trend(current_values)\n",
    "            seasonal = self._compute_seasonal(\n",
    "                current_values, seasonal_period, seasonal_strength\n",
    "            )\n",
    "\n",
    "            # MCMC step\n",
    "            current_values = self._mcmc_step(\n",
    "                current_values,\n",
    "                missing_mask,\n",
    "                trend,\n",
    "                seasonal,\n",
    "                anomaly_scores,\n",
    "                iteration,\n",
    "            )\n",
    "\n",
    "            # Collect samples\n",
    "            if iteration >= self.burn_in:\n",
    "                samples.append(current_values[missing_mask].copy())\n",
    "\n",
    "            if (iteration + 1) % 100 == 0:\n",
    "                print(f\"   Progress: {iteration + 1}/{self.n_iter}\")\n",
    "\n",
    "        # Step 6: Final values\n",
    "        if samples:\n",
    "            samples = np.array(samples)\n",
    "            final_values = current_values\n",
    "\n",
    "            for i, idx in enumerate(missing_indices):\n",
    "                final_values[idx] = np.mean(samples[:, i])\n",
    "        else:\n",
    "            final_values = current_values\n",
    "\n",
    "        print(f\"âœ… MCMC imputation completed\")\n",
    "\n",
    "        # Map back to original timestamps\n",
    "        result_values = np.full(len(timestamps), np.nan)\n",
    "        ts_np = timestamps.to_numpy()\n",
    "\n",
    "        for i, ts in enumerate(ts_np):\n",
    "            idx = np.argmin(np.abs(timestamps - ts))\n",
    "            if np.abs(timestamps[idx] - ts) < np.median(np.diff(timestamps)) / 2:\n",
    "                result_values[i] = final_values[idx]\n",
    "            else:\n",
    "                result_values[i] = (\n",
    "                    values[i] if not np.isnan(values[i]) else np.nanmean(final_values)\n",
    "                )\n",
    "\n",
    "        return pl.Series(\"imputed_value\", result_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbf27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC Imputation\n",
    "\n",
    "# Select a sample KPI for testing\n",
    "sample_kpi_id = \"05f10d3a-239c-3bef-9bdc-a2feeb0037aa\"  # Has 2.19% missing rate\n",
    "sample_data = train_ts.filter(pl.col(\"KPI ID\") == sample_kpi_id).sort(\"timestamp\")\n",
    "\n",
    "print(f\"ğŸ“Š Testing MCMC imputation on KPI: {sample_kpi_id}\")\n",
    "print(f\"   Original data points: {len(sample_data)}\")\n",
    "print(\n",
    "    f\"   Value range: [{sample_data['value'].min():.6f}, {sample_data['value'].max():.6f}]\"\n",
    ")\n",
    "\n",
    "# Use the clean MCMCKPIImputer implementation\n",
    "imputer = MCMCKPIImputer(\n",
    "    n_iter=200,  # Reduced for demo\n",
    "    burn_in=50,\n",
    "    use_spectral=True,\n",
    "    adaptive=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Perform imputation\n",
    "imputed_values = imputer.fit_transform(\n",
    "    timestamps=sample_data[\"timestamp\"], values=sample_data[\"value\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Imputation Results:\")\n",
    "print(f\"   Output points: {len(imputed_values)}\")\n",
    "print(\n",
    "    f\"   Imputed value range: [{imputed_values.min():.6f}, {imputed_values.max():.6f}]\"\n",
    ")\n",
    "\n",
    "# Create result dataframe\n",
    "result_df = pl.DataFrame(\n",
    "    {\n",
    "        \"timestamp\": sample_data[\"timestamp\"],\n",
    "        \"original_value\": sample_data[\"value\"],\n",
    "        \"imputed_value\": imputed_values,\n",
    "        \"kpi_id\": sample_data[\"KPI ID\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Show sample results\n",
    "print(f\"\\nğŸ” Sample of imputed data:\")\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def analyze_missing_intervals(df, kpi_id):\n",
    "    \"\"\"\n",
    "    åˆ†æå•ä¸ªKPIçš„ç¼ºå¤±åŒºé—´é•¿åº¦\n",
    "    \"\"\"\n",
    "    # è·å–æ—¶é—´æˆ³\n",
    "    timestamps = df[\"timestamp\"].to_numpy()\n",
    "\n",
    "    if len(timestamps) < 2:\n",
    "        return {\"kpi_id\": kpi_id, \"intervals\": [], \"stats\": {}}\n",
    "\n",
    "    # è®¡ç®—æ—¶é—´é—´éš”\n",
    "    intervals = np.diff(timestamps)\n",
    "    median_interval = int(np.median(intervals))\n",
    "\n",
    "    # æ‰¾åˆ°ç¼ºå¤±åŒºé—´\n",
    "    gap_threshold = median_interval * 1.5\n",
    "    missing_intervals = []\n",
    "\n",
    "    for i, interval in enumerate(intervals):\n",
    "        if interval > gap_threshold:\n",
    "            # è®¡ç®—ç¼ºå¤±çš„ç‚¹æ•°\n",
    "            missing_points = int(interval / median_interval) - 1\n",
    "            if missing_points > 0:\n",
    "                missing_intervals.append(\n",
    "                    {\n",
    "                        \"start_idx\": i,\n",
    "                        \"end_idx\": i + 1,\n",
    "                        \"start_time\": timestamps[i],\n",
    "                        \"end_time\": timestamps[i + 1],\n",
    "                        \"duration_seconds\": interval,\n",
    "                        \"missing_points\": missing_points,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯\n",
    "    if missing_intervals:\n",
    "        durations = [mi[\"duration_seconds\"] for mi in missing_intervals]\n",
    "        missing_points_list = [mi[\"missing_points\"] for mi in missing_intervals]\n",
    "\n",
    "        stats = {\n",
    "            \"total_gaps\": len(missing_intervals),\n",
    "            \"duration_min\": min(durations),\n",
    "            \"duration_max\": max(durations),\n",
    "            \"duration_mean\": np.mean(durations),\n",
    "            \"points_min\": min(missing_points_list),\n",
    "            \"points_max\": max(missing_points_list),\n",
    "            \"points_mean\": np.mean(missing_points_list),\n",
    "            \"total_missing_points\": sum(missing_points_list),\n",
    "        }\n",
    "    else:\n",
    "        stats = {\n",
    "            \"total_gaps\": 0,\n",
    "            \"duration_min\": 0,\n",
    "            \"duration_max\": 0,\n",
    "            \"duration_mean\": 0,\n",
    "            \"points_min\": 0,\n",
    "            \"points_max\": 0,\n",
    "            \"points_mean\": 0,\n",
    "            \"total_missing_points\": 0,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"kpi_id\": kpi_id,\n",
    "        \"intervals\": missing_intervals,\n",
    "        \"stats\": stats,\n",
    "        \"median_interval\": median_interval,\n",
    "    }\n",
    "\n",
    "\n",
    "# åˆ†ææ‰€æœ‰KPIçš„ç¼ºå¤±åŒºé—´\n",
    "print(\"ğŸ” åˆ†ææ‰€æœ‰KPIçš„ç¼ºå¤±åŒºé—´...\")\n",
    "all_kpi_analysis = []\n",
    "\n",
    "for kpi_id in train_ts[\"KPI ID\"].unique():\n",
    "    kpi_ts = train_ts.filter(pl.col(\"KPI ID\") == kpi_id).sort(\"timestamp\")\n",
    "    analysis = analyze_missing_intervals(kpi_ts, kpi_id)\n",
    "    all_kpi_analysis.append(analysis)\n",
    "\n",
    "# åˆ›å»ºç»Ÿè®¡æ±‡æ€»è¡¨\n",
    "summary_data = []\n",
    "for analysis in all_kpi_analysis:\n",
    "    stats = analysis[\"stats\"]\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"KPI ID\": analysis[\"kpi_id\"][:8] + \"...\",  # æˆªçŸ­æ˜¾ç¤º\n",
    "            \"Total Gaps\": stats[\"total_gaps\"],\n",
    "            \"Duration Min (s)\": stats[\"duration_min\"],\n",
    "            \"Duration Max (s)\": stats[\"duration_max\"],\n",
    "            \"Duration Mean (s)\": round(stats[\"duration_mean\"], 1),\n",
    "            \"Points Min\": stats[\"points_min\"],\n",
    "            \"Points Max\": stats[\"points_max\"],\n",
    "            \"Points Mean\": round(stats[\"points_mean\"], 1),\n",
    "            \"Total Missing\": stats[\"total_missing_points\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pl.DataFrame(summary_data)\n",
    "print(\"\\nğŸ“Š ç¼ºå¤±åŒºé—´ç»Ÿè®¡æ±‡æ€»:\")\n",
    "print(summary_df)\n",
    "\n",
    "# æ‰¾åˆ°æœ‰ç¼ºå¤±åŒºé—´çš„KPIè¿›è¡Œè¯¦ç»†åˆ†æ\n",
    "kpis_with_gaps = [a for a in all_kpi_analysis if a[\"stats\"][\"total_gaps\"] > 0]\n",
    "print(f\"\\nğŸ“ˆ æœ‰ç¼ºå¤±åŒºé—´çš„KPIæ•°é‡: {len(kpis_with_gaps)}\")\n",
    "\n",
    "if kpis_with_gaps:\n",
    "    # é€‰æ‹©ç¼ºå¤±åŒºé—´æœ€å¤šçš„KPIè¿›è¡Œå¯è§†åŒ–\n",
    "    selected_analysis = max(kpis_with_gaps, key=lambda x: x[\"stats\"][\"total_gaps\"])\n",
    "    selected_kpi = selected_analysis[\"kpi_id\"]\n",
    "\n",
    "    print(f\"\\nğŸ¯ é€‰æ‹©KPIè¿›è¡Œè¯¦ç»†å¯è§†åŒ–: {selected_kpi}\")\n",
    "    print(f\"   ç¼ºå¤±åŒºé—´æ•°é‡: {selected_analysis['stats']['total_gaps']}\")\n",
    "    print(f\"   æ€»ç¼ºå¤±ç‚¹æ•°: {selected_analysis['stats']['total_missing_points']}\")\n",
    "\n",
    "    # è·å–è¯¥KPIçš„å®Œæ•´æ•°æ®\n",
    "    kpi_ts = train_ts.filter(pl.col(\"KPI ID\") == selected_kpi).sort(\"timestamp\")\n",
    "\n",
    "    # é€‰æ‹©ä¸€ä¸ªæœ‰ä»£è¡¨æ€§çš„ç¼ºå¤±åŒºé—´è¿›è¡Œå¯è§†åŒ–\n",
    "    if selected_analysis[\"intervals\"]:\n",
    "        # é€‰æ‹©ç¼ºå¤±ç‚¹æ•°æœ€å¤šçš„åŒºé—´\n",
    "        target_interval = max(\n",
    "            selected_analysis[\"intervals\"], key=lambda x: x[\"missing_points\"]\n",
    "        )\n",
    "\n",
    "        print(f\"\\nğŸ“ é€‰æ‹©çš„ç¼ºå¤±åŒºé—´:\")\n",
    "        print(\n",
    "            f\"   ä½ç½®: ç´¢å¼• {target_interval['start_idx']} - {target_interval['end_idx']}\"\n",
    "        )\n",
    "        print(f\"   æŒç»­æ—¶é—´: {target_interval['duration_seconds']} ç§’\")\n",
    "        print(f\"   ç¼ºå¤±ç‚¹æ•°: {target_interval['missing_points']}\")\n",
    "\n",
    "        # ç¡®å®šå¯è§†åŒ–çª—å£\n",
    "        start_idx = max(0, target_interval[\"start_idx\"] - 50)  # å‰50ä¸ªç‚¹\n",
    "        end_idx = min(len(kpi_ts), target_interval[\"end_idx\"] + 50)  # å50ä¸ªç‚¹\n",
    "\n",
    "        viz_data = kpi_ts.slice(start_idx, end_idx - start_idx)\n",
    "\n",
    "        # è½¬æ¢ä¸ºnumpyæ•°ç»„ä¾¿äºå¤„ç†\n",
    "        timestamps = viz_data[\"timestamp\"].to_numpy()\n",
    "        values = viz_data[\"value\"].to_numpy()\n",
    "\n",
    "        # è½¬æ¢ä¸ºç›¸å¯¹æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰\n",
    "        start_time = timestamps[0]\n",
    "        relative_times = (timestamps - start_time) / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ\n",
    "\n",
    "        # åˆ›å»ºå¯è§†åŒ–\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "        # ä¸Šå›¾ï¼šåŸå§‹æ•°æ®æ˜¾ç¤ºç¼ºå¤±åŒºé—´\n",
    "        ax1.plot(\n",
    "            relative_times,\n",
    "            values,\n",
    "            \"b-\",\n",
    "            linewidth=1.5,\n",
    "            marker=\"o\",\n",
    "            markersize=3,\n",
    "            label=\"è§‚æµ‹æ•°æ®\",\n",
    "        )\n",
    "\n",
    "        # æ ‡è®°ç¼ºå¤±åŒºé—´\n",
    "        gap_start_time = target_interval[\"start_time\"]\n",
    "        gap_end_time = target_interval[\"end_time\"]\n",
    "        gap_start_rel = (gap_start_time - start_time) / 60\n",
    "        gap_end_rel = (gap_end_time - start_time) / 60\n",
    "\n",
    "        ax1.axvspan(\n",
    "            gap_start_rel,\n",
    "            gap_end_rel,\n",
    "            alpha=0.3,\n",
    "            color=\"red\",\n",
    "            label=f'ç¼ºå¤±åŒºé—´ ({target_interval[\"missing_points\"]}ä¸ªç‚¹)',\n",
    "        )\n",
    "        ax1.axvline(gap_start_rel, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "        ax1.axvline(gap_end_rel, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        ax1.set_title(f\"KPIç¼ºå¤±åŒºé—´å¯è§†åŒ–\\nKPI ID: {selected_kpi}\")\n",
    "        ax1.set_xlabel(\"ç›¸å¯¹æ—¶é—´ (åˆ†é’Ÿ)\")\n",
    "        ax1.set_ylabel(\"KPIå€¼\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # ä¸‹å›¾ï¼šæ’å€¼åçš„æ•ˆæœ\n",
    "        # åˆ›å»ºå®Œæ•´çš„æ—¶é—´åºåˆ—è¿›è¡Œæ’å€¼æ¼”ç¤º\n",
    "        interval = selected_analysis[\"median_interval\"]\n",
    "        complete_timestamps = np.arange(\n",
    "            timestamps[0], timestamps[-1] + interval, interval\n",
    "        )\n",
    "        complete_values = np.full(len(complete_timestamps), np.nan)\n",
    "\n",
    "        # å¡«å…¥å·²çŸ¥å€¼\n",
    "        for i, timestamp in enumerate(timestamps):\n",
    "            closest_idx = np.argmin(np.abs(complete_timestamps - timestamp))\n",
    "            if np.abs(complete_timestamps[closest_idx] - timestamp) < interval / 2:\n",
    "                complete_values[closest_idx] = values[i]\n",
    "\n",
    "        # çº¿æ€§æ’å€¼\n",
    "        mask = ~np.isnan(complete_values)\n",
    "        complete_values_interp = np.interp(\n",
    "            complete_timestamps, complete_timestamps[mask], complete_values[mask]\n",
    "        )\n",
    "\n",
    "        complete_times_rel = (complete_timestamps - start_time) / 60\n",
    "\n",
    "        ax2.plot(\n",
    "            complete_times_rel,\n",
    "            complete_values_interp,\n",
    "            \"g-\",\n",
    "            linewidth=1.5,\n",
    "            label=\"çº¿æ€§æ’å€¼\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "        ax2.plot(relative_times, values, \"bo\", markersize=4, label=\"åŸå§‹è§‚æµ‹ç‚¹\")\n",
    "\n",
    "        # æ ‡è®°æ’å€¼åŒºé—´\n",
    "        ax2.axvspan(\n",
    "            gap_start_rel, gap_end_rel, alpha=0.2, color=\"green\", label=\"æ’å€¼åŒºé—´\"\n",
    "        )\n",
    "\n",
    "        ax2.set_title(\"çº¿æ€§æ’å€¼å¡«å……æ•ˆæœ\")\n",
    "        ax2.set_xlabel(\"ç›¸å¯¹æ—¶é—´ (åˆ†é’Ÿ)\")\n",
    "        ax2.set_ylabel(\"KPIå€¼\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # æ˜¾ç¤ºç¼ºå¤±åŒºé—´çš„è¯¦ç»†ç»Ÿè®¡\n",
    "        print(f\"\\nğŸ“Š æ‰€æœ‰ç¼ºå¤±åŒºé—´è¯¦æƒ…:\")\n",
    "        for i, interval in enumerate(selected_analysis[\"intervals\"][:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª\n",
    "            duration_min = interval[\"duration_seconds\"] / 60\n",
    "            print(\n",
    "                f\"   åŒºé—´ {i+1}: {interval['missing_points']}ä¸ªç‚¹, {duration_min:.1f}åˆ†é’Ÿ\"\n",
    "            )\n",
    "\n",
    "        if len(selected_analysis[\"intervals\"]) > 5:\n",
    "            print(f\"   ... è¿˜æœ‰ {len(selected_analysis['intervals']) - 5} ä¸ªåŒºé—´\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰å‘ç°æ˜æ˜¾çš„ç¼ºå¤±åŒºé—´\")\n",
    "\n",
    "# å…¨å±€ç»Ÿè®¡\n",
    "total_gaps = sum(a[\"stats\"][\"total_gaps\"] for a in all_kpi_analysis)\n",
    "total_missing_points = sum(a[\"stats\"][\"total_missing_points\"] for a in all_kpi_analysis)\n",
    "\n",
    "print(f\"\\nğŸŒ å…¨å±€ç»Ÿè®¡:\")\n",
    "print(f\"   æ€»KPIæ•°é‡: {len(all_kpi_analysis)}\")\n",
    "print(f\"   æœ‰ç¼ºå¤±çš„KPIæ•°é‡: {len(kpis_with_gaps)}\")\n",
    "print(f\"   æ€»ç¼ºå¤±åŒºé—´æ•°: {total_gaps}\")\n",
    "print(f\"   æ€»ç¼ºå¤±ç‚¹æ•°: {total_missing_points}\")\n",
    "\n",
    "if kpis_with_gaps:\n",
    "    all_durations = []\n",
    "    all_points = []\n",
    "    for analysis in kpis_with_gaps:\n",
    "        for interval in analysis[\"intervals\"]:\n",
    "            all_durations.append(interval[\"duration_seconds\"])\n",
    "            all_points.append(interval[\"missing_points\"])\n",
    "\n",
    "    print(f\"\\nğŸ“ˆ ç¼ºå¤±åŒºé—´é•¿åº¦ç»Ÿè®¡:\")\n",
    "    print(\n",
    "        f\"   æŒç»­æ—¶é—´ - æœ€å°å€¼: {min(all_durations)}ç§’, æœ€å¤§å€¼: {max(all_durations)}ç§’, å‡å€¼: {np.mean(all_durations):.1f}ç§’\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ç¼ºå¤±ç‚¹æ•° - æœ€å°å€¼: {min(all_points)}ä¸ª, æœ€å¤§å€¼: {max(all_points)}ä¸ª, å‡å€¼: {np.mean(all_points):.1f}ä¸ª\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92dfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤„ç†æµ‹è¯•æ•°æ®å¹¶æŒ‰KPI IDåˆ†å‰²ä¿å­˜\n",
    "def process_and_split_by_kpi():\n",
    "    \"\"\"å¤„ç†æµ‹è¯•æ•°æ®å¹¶æŒ‰KPI IDåˆ†å‰²æˆå•ç‹¬çš„CSVæ–‡ä»¶\"\"\"\n",
    "    if test_ts is None:\n",
    "        print(\"âŒ æµ‹è¯•æ•°æ®æœªåŠ è½½ï¼Œæ— æ³•å¤„ç†\")\n",
    "        return None\n",
    "\n",
    "    print(\"å¼€å§‹å¤„ç†æµ‹è¯•æ•°æ®å¹¶æŒ‰KPI IDåˆ†å‰²...\")\n",
    "    print(f\"åŸå§‹æ•°æ®: {len(test_ts):,} è¡Œ\")\n",
    "\n",
    "    # è·å–æ‰€æœ‰KPI ID\n",
    "    kpi_ids = test_ts[\"KPI ID\"].unique()\n",
    "    print(f\"KPIæ•°é‡: {len(kpi_ids)}\")\n",
    "\n",
    "    output_dir = \"../datasets/kpi/train\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"è¾“å‡ºç›®å½•: {output_dir}\")\n",
    "\n",
    "    processed_count = 0\n",
    "    failed_count = 0\n",
    "    processing_stats = []\n",
    "\n",
    "    for i, kpi_id in enumerate(kpi_ids, 1):\n",
    "        print(f\"\\n[{i}/{len(kpi_ids)}] å¤„ç† KPI: {str(kpi_id)[:50]}...\")\n",
    "\n",
    "        try:\n",
    "            # ä½¿ç”¨æ›´é«˜æ•ˆçš„è¿‡æ»¤æ–¹æ³•\n",
    "            kpi_mask = test_ts[\"KPI ID\"] == kpi_id\n",
    "            kpi_data = (\n",
    "                test_ts[kpi_mask].copy().sort_values(\"timestamp\").reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            original_length = len(kpi_data)\n",
    "            print(f\"  åŸå§‹é•¿åº¦: {original_length}\")\n",
    "\n",
    "            if original_length == 0:\n",
    "                print(f\"  âš ï¸ è·³è¿‡ç©ºKPI\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "\n",
    "            # æ£€æŸ¥æ—¶é—´é—´éš”\n",
    "            missing_filled = 0\n",
    "            if len(kpi_data) > 1:\n",
    "                time_diffs = np.diff(kpi_data[\"timestamp\"].values)\n",
    "                interval = np.median(time_diffs)\n",
    "                print(f\"  æ—¶é—´é—´éš”: {interval} ç§’\")\n",
    "\n",
    "                # ç®€å•çš„ç¼ºå¤±ç‚¹æ£€æµ‹\n",
    "                expected_length = (\n",
    "                    int(\n",
    "                        (kpi_data[\"timestamp\"].iloc[-1] - kpi_data[\"timestamp\"].iloc[0])\n",
    "                        / interval\n",
    "                    )\n",
    "                    + 1\n",
    "                )\n",
    "                missing_count = expected_length - len(kpi_data)\n",
    "                print(f\"  é¢„æœŸé•¿åº¦: {expected_length}, ç¼ºå¤±ç‚¹æ•°: {missing_count}\")\n",
    "\n",
    "                # æ’å€¼\n",
    "                if (\n",
    "                    missing_count > 0 and missing_count < len(kpi_data) * 0.2\n",
    "                ):  # ç¼ºå¤±ç‚¹å°‘äº20%\n",
    "                    # åˆ›å»ºå®Œæ•´çš„æ—¶é—´åºåˆ—\n",
    "                    start_time = kpi_data[\"timestamp\"].iloc[0]\n",
    "                    end_time = kpi_data[\"timestamp\"].iloc[-1]\n",
    "                    complete_timestamps = np.arange(\n",
    "                        start_time, end_time + interval, interval\n",
    "                    )\n",
    "\n",
    "                    # åˆ›å»ºå®Œæ•´çš„DataFrame\n",
    "                    complete_df = pd.DataFrame(\n",
    "                        {\"timestamp\": complete_timestamps, \"KPI ID\": kpi_id}\n",
    "                    )\n",
    "\n",
    "                    # åˆå¹¶æ•°æ®\n",
    "                    merged_df = complete_df.merge(\n",
    "                        kpi_data, on=[\"timestamp\", \"KPI ID\"], how=\"left\"\n",
    "                    )\n",
    "\n",
    "                    # çº¿æ€§æ’å€¼\n",
    "                    merged_df[\"value\"] = merged_df[\"value\"].interpolate(method=\"linear\")\n",
    "                    merged_df[\"label\"] = merged_df[\"label\"].fillna(0).astype(int)\n",
    "\n",
    "                    kpi_data = merged_df\n",
    "                    missing_filled = len(kpi_data) - original_length\n",
    "                    print(f\"  æ’å€¼åé•¿åº¦: {len(kpi_data)}, å¡«è¡¥ç‚¹æ•°: {missing_filled}\")\n",
    "\n",
    "            # ç¡®ä¿é•¿åº¦æ˜¯32çš„å€æ•°\n",
    "            current_length = len(kpi_data)\n",
    "            target_length = (current_length // 32) * 32\n",
    "\n",
    "            if target_length < 32:\n",
    "                print(f\"  âš ï¸ è·³è¿‡KPI {kpi_id} (æ•°æ®å¤ªå°‘ï¼Œå°‘äº32ä¸ªç‚¹)\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "\n",
    "            # æˆªå–åˆ°32çš„å€æ•°é•¿åº¦\n",
    "            kpi_data = kpi_data.iloc[:target_length]\n",
    "            final_length = len(kpi_data)\n",
    "            points_removed = current_length - final_length\n",
    "\n",
    "            print(f\"  æœ€ç»ˆé•¿åº¦: {final_length} (32çš„å€æ•°: {final_length % 32 == 0})\")\n",
    "            print(f\"  ç§»é™¤ç‚¹æ•°: {points_removed}\")\n",
    "\n",
    "            # å»æ‰KPI IDåˆ—ï¼Œåªä¿ç•™timestamp, value, label\n",
    "            kpi_data_clean = kpi_data[[\"timestamp\", \"value\", \"label\"]].copy()\n",
    "\n",
    "            # ä¿å­˜ä¸ºå•ç‹¬çš„CSVæ–‡ä»¶ï¼Œæ–‡ä»¶åä¸ºKPI ID\n",
    "            # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦\n",
    "            safe_filename = (\n",
    "                str(kpi_id).replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "            )\n",
    "            output_file = os.path.join(output_dir, f\"{safe_filename}.csv\")\n",
    "\n",
    "            kpi_data_clean.to_csv(output_file, index=False)\n",
    "            print(f\"  âœ… å·²ä¿å­˜åˆ°: {safe_filename}.csv\")\n",
    "\n",
    "            # è®°å½•ç»Ÿè®¡ä¿¡æ¯\n",
    "            processing_stats.append(\n",
    "                {\n",
    "                    \"KPI_ID\": kpi_id,\n",
    "                    \"filename\": f\"{safe_filename}.csv\",\n",
    "                    \"original_length\": original_length,\n",
    "                    \"final_length\": final_length,\n",
    "                    \"missing_filled\": missing_filled,\n",
    "                    \"points_removed\": points_removed,\n",
    "                    \"anomaly_count\": kpi_data_clean[\"label\"].sum(),\n",
    "                    \"anomaly_rate\": kpi_data_clean[\"label\"].mean(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ å¤„ç†KPI {kpi_id} æ—¶å‡ºé”™: {e}\")\n",
    "            failed_count += 1\n",
    "            continue\n",
    "\n",
    "    # ç”Ÿæˆå¤„ç†ç»Ÿè®¡æŠ¥å‘Š\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"å¤„ç†å®Œæˆç»Ÿè®¡\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"æ€»KPIæ•°é‡: {len(kpi_ids)}\")\n",
    "    print(f\"æˆåŠŸå¤„ç†: {processed_count}\")\n",
    "    print(f\"å¤„ç†å¤±è´¥: {failed_count}\")\n",
    "    print(f\"æˆåŠŸç‡: {processed_count/len(kpi_ids)*100:.1f}%\")\n",
    "\n",
    "    if processing_stats:\n",
    "        stats_df = pd.DataFrame(processing_stats)\n",
    "\n",
    "        print(f\"\\nè¯¦ç»†ç»Ÿè®¡:\")\n",
    "        print(f\"  å¹³å‡åºåˆ—é•¿åº¦: {stats_df['final_length'].mean():.0f}\")\n",
    "        print(f\"  æœ€çŸ­åºåˆ—: {stats_df['final_length'].min():,}\")\n",
    "        print(f\"  æœ€é•¿åºåˆ—: {stats_df['final_length'].max():,}\")\n",
    "        print(f\"  æ€»æ•°æ®ç‚¹: {stats_df['final_length'].sum():,}\")\n",
    "        print(f\"  æ€»å¼‚å¸¸ç‚¹: {stats_df['anomaly_count'].sum():,}\")\n",
    "        print(f\"  å¹³å‡å¼‚å¸¸ç‡: {stats_df['anomaly_rate'].mean()*100:.2f}%\")\n",
    "\n",
    "        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯\n",
    "        stats_file = os.path.join(output_dir, \"processing_statistics.csv\")\n",
    "        stats_df.to_csv(stats_file, index=False)\n",
    "        print(f\"\\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: processing_statistics.csv\")\n",
    "\n",
    "        # æ˜¾ç¤ºåºåˆ—é•¿åº¦åˆ†å¸ƒ\n",
    "        length_counts = stats_df[\"final_length\"].value_counts().sort_index()\n",
    "        print(f\"\\nåºåˆ—é•¿åº¦åˆ†å¸ƒ:\")\n",
    "        for length, count in length_counts.head(10).items():\n",
    "            print(f\"  é•¿åº¦ {length:,}: {count} ä¸ªKPI\")\n",
    "        if len(length_counts) > 10:\n",
    "            print(f\"  ... è¿˜æœ‰ {len(length_counts)-10} ç§é•¿åº¦\")\n",
    "\n",
    "        return stats_df\n",
    "    else:\n",
    "        print(\"âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•KPI\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# æ‰§è¡Œå¤„ç†\n",
    "print(\"å¼€å§‹æŒ‰KPI IDåˆ†å‰²å¤„ç†...\")\n",
    "processing_result = process_and_split_by_kpi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯ç”Ÿæˆçš„CSVæ–‡ä»¶\n",
    "def validate_generated_files():\n",
    "    \"\"\"éªŒè¯ç”Ÿæˆçš„CSVæ–‡ä»¶\"\"\"\n",
    "    output_dir = \"../datasets/kpi/train\"\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(\"âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨\")\n",
    "        return\n",
    "\n",
    "    # è·å–æ‰€æœ‰CSVæ–‡ä»¶\n",
    "    csv_files = [\n",
    "        f\n",
    "        for f in os.listdir(output_dir)\n",
    "        if f.endswith(\".csv\") and f != \"processing_statistics.csv\"\n",
    "    ]\n",
    "\n",
    "    print(f\"éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶...\")\n",
    "    print(f\"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶\")\n",
    "\n",
    "    if len(csv_files) == 0:\n",
    "        print(\"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶\")\n",
    "        return\n",
    "\n",
    "    # éšæœºæ£€æŸ¥å‡ ä¸ªæ–‡ä»¶\n",
    "    import random\n",
    "\n",
    "    sample_files = random.sample(csv_files, min(5, len(csv_files)))\n",
    "\n",
    "    print(f\"\\néšæœºæ£€æŸ¥ {len(sample_files)} ä¸ªæ–‡ä»¶:\")\n",
    "\n",
    "    for filename in sample_files:\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            print(f\"\\nğŸ“ {filename}:\")\n",
    "            print(f\"  å½¢çŠ¶: {df.shape}\")\n",
    "            print(f\"  åˆ—å: {df.columns.tolist()}\")\n",
    "            print(f\"  é•¿åº¦æ˜¯32çš„å€æ•°: {len(df) % 32 == 0}\")\n",
    "            print(f\"  å¼‚å¸¸ç‚¹æ•°: {df['label'].sum()}\")\n",
    "            print(f\"  å¼‚å¸¸ç‡: {df['label'].mean()*100:.2f}%\")\n",
    "\n",
    "            # æ£€æŸ¥æ˜¯å¦æœ‰KPI IDåˆ—\n",
    "            if \"KPI ID\" in df.columns:\n",
    "                print(f\"  âš ï¸ è­¦å‘Š: æ–‡ä»¶ä¸­ä»åŒ…å«KPI IDåˆ—\")\n",
    "            else:\n",
    "                print(f\"  âœ… å·²æ­£ç¡®ç§»é™¤KPI IDåˆ—\")\n",
    "\n",
    "            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\n",
    "            if df.isnull().sum().sum() > 0:\n",
    "                print(f\"  âš ï¸ è­¦å‘Š: å­˜åœ¨ç¼ºå¤±å€¼\")\n",
    "                print(f\"  ç¼ºå¤±å€¼ç»Ÿè®¡: {df.isnull().sum().to_dict()}\")\n",
    "            else:\n",
    "                print(f\"  âœ… æ— ç¼ºå¤±å€¼\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}\")\n",
    "\n",
    "    print(f\"\\nğŸ“Š æ€»ä½“ç»Ÿè®¡:\")\n",
    "    print(f\"  ç”Ÿæˆæ–‡ä»¶æ•°: {len(csv_files)}\")\n",
    "    print(f\"  è¾“å‡ºç›®å½•: {output_dir}\")\n",
    "\n",
    "    # æ£€æŸ¥ç»Ÿè®¡æ–‡ä»¶\n",
    "    stats_file = os.path.join(output_dir, \"processing_statistics.csv\")\n",
    "    if os.path.exists(stats_file):\n",
    "        stats_df = pd.read_csv(stats_file)\n",
    "        print(f\"  ç»Ÿè®¡æ–‡ä»¶: âœ… (åŒ…å« {len(stats_df)} æ¡è®°å½•)\")\n",
    "\n",
    "        # æ˜¾ç¤ºä¸€äº›å…³é”®ç»Ÿè®¡\n",
    "        print(f\"\\nå…³é”®ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "        print(f\"  æ€»æ•°æ®ç‚¹: {stats_df['final_length'].sum():,}\")\n",
    "        print(f\"  å¹³å‡åºåˆ—é•¿åº¦: {stats_df['final_length'].mean():.0f}\")\n",
    "        print(f\"  æ€»å¼‚å¸¸ç‚¹: {stats_df['anomaly_count'].sum():,}\")\n",
    "        print(f\"  å¹³å‡å¼‚å¸¸ç‡: {stats_df['anomaly_rate'].mean()*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"  ç»Ÿè®¡æ–‡ä»¶: âŒ ä¸å­˜åœ¨\")\n",
    "\n",
    "\n",
    "# æ‰§è¡ŒéªŒè¯\n",
    "try:\n",
    "    if processing_result is not None:\n",
    "        validate_generated_files()\n",
    "    else:\n",
    "        print(\"è·³è¿‡éªŒè¯ï¼Œå› ä¸ºå¤„ç†æœªæˆåŠŸå®Œæˆ\")\n",
    "except NameError:\n",
    "    print(\"processing_result å˜é‡æœªå®šä¹‰ï¼Œè¯·å…ˆè¿è¡Œå¤„ç†å‡½æ•°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMCæ–¹æ³•å¤„ç†ç¼ºå¤±ç‚¹ - åŸºäºVAEè®ºæ–‡çš„å®ç°\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "class MCMCMissingValueImputer:\n",
    "    \"\"\"\n",
    "    åŸºäºMCMCçš„ç¼ºå¤±å€¼æ’è¡¥å™¨\n",
    "    å‚è€ƒ: Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_iterations: int = 1000,\n",
    "        burn_in: int = 200,\n",
    "        seasonal_period: Optional[int] = None,\n",
    "        trend_strength: float = 0.1,\n",
    "        seasonal_strength: float = 0.3,\n",
    "        noise_variance: float = 0.01,\n",
    "        random_state: int = 42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "        - n_iterations: MCMCè¿­ä»£æ¬¡æ•°\n",
    "        - burn_in: burn-inæœŸé—´ï¼Œä¸¢å¼ƒçš„åˆå§‹æ ·æœ¬æ•°\n",
    "        - seasonal_period: å­£èŠ‚å‘¨æœŸï¼ˆå¦‚æœNoneåˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰\n",
    "        - trend_strength: è¶‹åŠ¿é¡¹å¼ºåº¦\n",
    "        - seasonal_strength: å­£èŠ‚é¡¹å¼ºåº¦\n",
    "        - noise_variance: å™ªå£°æ–¹å·®\n",
    "        \"\"\"\n",
    "        self.n_iterations = n_iterations\n",
    "        self.burn_in = burn_in\n",
    "        self.seasonal_period = seasonal_period\n",
    "        self.trend_strength = trend_strength\n",
    "        self.seasonal_strength = seasonal_strength\n",
    "        self.noise_variance = noise_variance\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    def _detect_seasonality(self, values: np.ndarray, timestamps: np.ndarray) -> int:\n",
    "        \"\"\"è‡ªåŠ¨æ£€æµ‹å­£èŠ‚å‘¨æœŸ\"\"\"\n",
    "        if len(values) < 50:\n",
    "            return min(24, len(values) // 4)  # é»˜è®¤24å°æ—¶å‘¨æœŸ\n",
    "\n",
    "        # ä½¿ç”¨FFTæ£€æµ‹ä¸»è¦é¢‘ç‡\n",
    "        from scipy.fft import fft, fftfreq\n",
    "\n",
    "        # å»é™¤è¶‹åŠ¿\n",
    "        detrended = values - np.linspace(values[0], values[-1], len(values))\n",
    "\n",
    "        # FFTåˆ†æ\n",
    "        fft_vals = np.abs(fft(detrended))\n",
    "        freqs = fftfreq(len(detrended))\n",
    "\n",
    "        # æ‰¾åˆ°æœ€å¼ºçš„é¢‘ç‡ï¼ˆæ’é™¤DCåˆ†é‡ï¼‰\n",
    "        fft_vals[0] = 0  # ç§»é™¤DCåˆ†é‡\n",
    "        peak_freq_idx = np.argmax(fft_vals[: len(fft_vals) // 2])\n",
    "\n",
    "        if freqs[peak_freq_idx] > 0:\n",
    "            period = int(1 / freqs[peak_freq_idx])\n",
    "            # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…\n",
    "            period = max(4, min(period, len(values) // 4))\n",
    "        else:\n",
    "            period = 24  # é»˜è®¤å€¼\n",
    "\n",
    "        return period\n",
    "\n",
    "    def _initialize_missing_values(\n",
    "        self, values: np.ndarray, missing_mask: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"åˆå§‹åŒ–ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨çº¿æ€§æ’å€¼ï¼‰\"\"\"\n",
    "        result = values.copy()\n",
    "        missing_indices = np.where(missing_mask)[0]\n",
    "\n",
    "        for idx in missing_indices:\n",
    "            # æ‰¾åˆ°æœ€è¿‘çš„éç¼ºå¤±å€¼\n",
    "            left_idx = idx - 1\n",
    "            right_idx = idx + 1\n",
    "\n",
    "            # å‘å·¦æœç´¢\n",
    "            while left_idx >= 0 and missing_mask[left_idx]:\n",
    "                left_idx -= 1\n",
    "\n",
    "            # å‘å³æœç´¢\n",
    "            while right_idx < len(values) and missing_mask[right_idx]:\n",
    "                right_idx += 1\n",
    "\n",
    "            # çº¿æ€§æ’å€¼\n",
    "            if left_idx >= 0 and right_idx < len(values):\n",
    "                # ä¸¤ä¾§éƒ½æœ‰å€¼\n",
    "                weight = (idx - left_idx) / (right_idx - left_idx)\n",
    "                result[idx] = (\n",
    "                    values[left_idx] * (1 - weight) + values[right_idx] * weight\n",
    "                )\n",
    "            elif left_idx >= 0:\n",
    "                # åªæœ‰å·¦ä¾§æœ‰å€¼\n",
    "                result[idx] = values[left_idx]\n",
    "            elif right_idx < len(values):\n",
    "                # åªæœ‰å³ä¾§æœ‰å€¼\n",
    "                result[idx] = values[right_idx]\n",
    "            else:\n",
    "                # éƒ½æ²¡æœ‰å€¼ï¼Œä½¿ç”¨å‡å€¼\n",
    "                result[idx] = np.nanmean(values[~missing_mask])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _compute_seasonal_component(\n",
    "        self, values: np.ndarray, period: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"è®¡ç®—å­£èŠ‚æ€§åˆ†é‡\"\"\"\n",
    "        n = len(values)\n",
    "        seasonal = np.zeros(n)\n",
    "\n",
    "        # å¯¹æ¯ä¸ªå­£èŠ‚ä½ç½®è®¡ç®—å¹³å‡å€¼\n",
    "        for i in range(period):\n",
    "            indices = np.arange(i, n, period)\n",
    "            if len(indices) > 0:\n",
    "                seasonal_value = np.mean(values[indices])\n",
    "                seasonal[indices] = seasonal_value\n",
    "\n",
    "        # ä¸­å¿ƒåŒ–å­£èŠ‚åˆ†é‡\n",
    "        seasonal = seasonal - np.mean(seasonal)\n",
    "        return seasonal\n",
    "\n",
    "    def _compute_trend_component(self, values: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"è®¡ç®—è¶‹åŠ¿åˆ†é‡ï¼ˆä½¿ç”¨ç§»åŠ¨å¹³å‡ï¼‰\"\"\"\n",
    "        window_size = min(len(values) // 4, 10)\n",
    "        if window_size < 3:\n",
    "            return np.full_like(values, np.mean(values))\n",
    "\n",
    "        # ç®€å•ç§»åŠ¨å¹³å‡\n",
    "        trend = np.convolve(values, np.ones(window_size) / window_size, mode=\"same\")\n",
    "\n",
    "        # å¤„ç†è¾¹ç•Œ\n",
    "        for i in range(window_size // 2):\n",
    "            trend[i] = np.mean(values[: i + window_size // 2 + 1])\n",
    "            trend[-(i + 1)] = np.mean(values[-(i + window_size // 2 + 1) :])\n",
    "\n",
    "        return trend\n",
    "\n",
    "    def _gibbs_sampling_step(\n",
    "        self,\n",
    "        values: np.ndarray,\n",
    "        missing_mask: np.ndarray,\n",
    "        trend: np.ndarray,\n",
    "        seasonal: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Gibbsé‡‡æ ·æ­¥éª¤\"\"\"\n",
    "        result = values.copy()\n",
    "        missing_indices = np.where(missing_mask)[0]\n",
    "\n",
    "        for idx in missing_indices:\n",
    "            # è®¡ç®—æ¡ä»¶åˆ†å¸ƒçš„å‚æ•°\n",
    "            # åŸºäºè¶‹åŠ¿å’Œå­£èŠ‚åˆ†é‡çš„å…ˆéªŒå‡å€¼\n",
    "            prior_mean = trend[idx] + seasonal[idx]\n",
    "\n",
    "            # åŸºäºé‚»è¿‘è§‚æµ‹å€¼çš„ä¼¼ç„¶\n",
    "            neighbors = []\n",
    "            neighbor_weights = []\n",
    "\n",
    "            # æ”¶é›†é‚»è¿‘çš„è§‚æµ‹å€¼\n",
    "            for offset in [-2, -1, 1, 2]:\n",
    "                neighbor_idx = idx + offset\n",
    "                if 0 <= neighbor_idx < len(values) and not missing_mask[neighbor_idx]:\n",
    "                    neighbors.append(values[neighbor_idx])\n",
    "                    # è·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§\n",
    "                    neighbor_weights.append(1.0 / abs(offset))\n",
    "\n",
    "            if neighbors:\n",
    "                neighbors = np.array(neighbors)\n",
    "                neighbor_weights = np.array(neighbor_weights)\n",
    "                neighbor_weights /= np.sum(neighbor_weights)\n",
    "\n",
    "                # åŠ æƒå¹³å‡ä½œä¸ºä¼¼ç„¶å‡å€¼\n",
    "                likelihood_mean = np.sum(neighbors * neighbor_weights)\n",
    "\n",
    "                # ç»“åˆå…ˆéªŒå’Œä¼¼ç„¶\n",
    "                # ä½¿ç”¨è´å¶æ–¯æ›´æ–°\n",
    "                prior_precision = 1.0 / (self.trend_strength + self.seasonal_strength)\n",
    "                likelihood_precision = len(neighbors) / self.noise_variance\n",
    "\n",
    "                posterior_precision = prior_precision + likelihood_precision\n",
    "                posterior_mean = (\n",
    "                    prior_precision * prior_mean\n",
    "                    + likelihood_precision * likelihood_mean\n",
    "                ) / posterior_precision\n",
    "                posterior_variance = 1.0 / posterior_precision\n",
    "            else:\n",
    "                # æ²¡æœ‰é‚»è¿‘è§‚æµ‹å€¼ï¼Œåªä½¿ç”¨å…ˆéªŒ\n",
    "                posterior_mean = prior_mean\n",
    "                posterior_variance = self.trend_strength + self.seasonal_strength\n",
    "\n",
    "            # ä»åéªŒåˆ†å¸ƒé‡‡æ ·\n",
    "            result[idx] = np.random.normal(posterior_mean, np.sqrt(posterior_variance))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def fit_transform(\n",
    "        self,\n",
    "        timestamps: np.ndarray,\n",
    "        values: np.ndarray,\n",
    "        missing_mask: Optional[np.ndarray] = None,\n",
    "    ) -> Tuple[np.ndarray, dict]:\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨MCMCæ–¹æ³•æ’è¡¥ç¼ºå¤±å€¼\n",
    "\n",
    "        å‚æ•°:\n",
    "        - timestamps: æ—¶é—´æˆ³æ•°ç»„\n",
    "        - values: å€¼æ•°ç»„ï¼ˆå¯åŒ…å«NaNï¼‰\n",
    "        - missing_mask: ç¼ºå¤±å€¼æ©ç ï¼ˆTrueè¡¨ç¤ºç¼ºå¤±ï¼‰\n",
    "\n",
    "        è¿”å›:\n",
    "        - æ’è¡¥åçš„å€¼æ•°ç»„\n",
    "        - è¯Šæ–­ä¿¡æ¯å­—å…¸\n",
    "        \"\"\"\n",
    "        if missing_mask is None:\n",
    "            missing_mask = np.isnan(values)\n",
    "\n",
    "        if not np.any(missing_mask):\n",
    "            return values.copy(), {\"message\": \"æ²¡æœ‰ç¼ºå¤±å€¼éœ€è¦å¤„ç†\"}\n",
    "\n",
    "        print(f\"å¼€å§‹MCMCæ’è¡¥ï¼Œç¼ºå¤±å€¼æ•°é‡: {np.sum(missing_mask)}\")\n",
    "\n",
    "        # 1. åˆå§‹åŒ–ç¼ºå¤±å€¼\n",
    "        current_values = self._initialize_missing_values(values, missing_mask)\n",
    "\n",
    "        # 2. æ£€æµ‹å­£èŠ‚å‘¨æœŸ\n",
    "        if self.seasonal_period is None:\n",
    "            observed_values = values[~missing_mask]\n",
    "            observed_timestamps = timestamps[~missing_mask]\n",
    "            self.seasonal_period = self._detect_seasonality(\n",
    "                observed_values, observed_timestamps\n",
    "            )\n",
    "\n",
    "        print(f\"æ£€æµ‹åˆ°çš„å­£èŠ‚å‘¨æœŸ: {self.seasonal_period}\")\n",
    "\n",
    "        # 3. MCMCé‡‡æ ·\n",
    "        samples = []\n",
    "\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # è®¡ç®—å½“å‰çš„è¶‹åŠ¿å’Œå­£èŠ‚åˆ†é‡\n",
    "            trend = self._compute_trend_component(current_values)\n",
    "            seasonal = self._compute_seasonal_component(\n",
    "                current_values, self.seasonal_period\n",
    "            )\n",
    "\n",
    "            # Gibbsé‡‡æ ·æ­¥éª¤\n",
    "            current_values = self._gibbs_sampling_step(\n",
    "                current_values, missing_mask, trend, seasonal\n",
    "            )\n",
    "\n",
    "            # ä¿å­˜æ ·æœ¬ï¼ˆburn-inåï¼‰\n",
    "            if iteration >= self.burn_in:\n",
    "                samples.append(current_values[missing_mask].copy())\n",
    "\n",
    "            if (iteration + 1) % 200 == 0:\n",
    "                print(f\"å®Œæˆè¿­ä»£ {iteration + 1}/{self.n_iterations}\")\n",
    "\n",
    "        # 4. è®¡ç®—æœ€ç»ˆç»“æœ\n",
    "        samples = np.array(samples)\n",
    "        final_values = values.copy()\n",
    "\n",
    "        # ä½¿ç”¨é‡‡æ ·å‡å€¼ä½œä¸ºæœ€ç»ˆä¼°è®¡\n",
    "        missing_indices = np.where(missing_mask)[0]\n",
    "        for i, idx in enumerate(missing_indices):\n",
    "            final_values[idx] = np.mean(samples[:, i])\n",
    "\n",
    "        # 5. è®¡ç®—è¯Šæ–­ä¿¡æ¯\n",
    "        diagnostics = {\n",
    "            \"n_missing\": np.sum(missing_mask),\n",
    "            \"seasonal_period\": self.seasonal_period,\n",
    "            \"n_samples\": len(samples),\n",
    "            \"sample_std\": np.std(samples, axis=0),\n",
    "            \"convergence_diagnostic\": self._compute_convergence_diagnostic(samples),\n",
    "        }\n",
    "\n",
    "        return final_values, diagnostics\n",
    "\n",
    "    def _compute_convergence_diagnostic(self, samples: np.ndarray) -> dict:\n",
    "        \"\"\"è®¡ç®—æ”¶æ•›è¯Šæ–­\"\"\"\n",
    "        if len(samples) < 100:\n",
    "            return {\"status\": \"æ ·æœ¬æ•°ä¸è¶³\"}\n",
    "\n",
    "        # åˆ†å‰²æ ·æœ¬ä¸ºä¸¤åŠ\n",
    "        mid = len(samples) // 2\n",
    "        first_half = samples[:mid]\n",
    "        second_half = samples[mid:]\n",
    "\n",
    "        # è®¡ç®—æ¯ä¸ªç¼ºå¤±ä½ç½®çš„Gelman-Rubinç»Ÿè®¡é‡\n",
    "        gr_stats = []\n",
    "        for i in range(samples.shape[1]):\n",
    "            if len(first_half) > 10 and len(second_half) > 10:\n",
    "                # è®¡ç®—é“¾å†…å’Œé“¾é—´æ–¹å·®\n",
    "                w = (np.var(first_half[:, i]) + np.var(second_half[:, i])) / 2\n",
    "                b = (\n",
    "                    len(first_half)\n",
    "                    * (np.mean(first_half[:, i]) - np.mean(second_half[:, i])) ** 2\n",
    "                )\n",
    "\n",
    "                if w > 0:\n",
    "                    gr = np.sqrt((w + b / len(first_half)) / w)\n",
    "                    gr_stats.append(gr)\n",
    "\n",
    "        if gr_stats:\n",
    "            max_gr = np.max(gr_stats)\n",
    "            converged = max_gr < 1.1  # é€šå¸¸çš„æ”¶æ•›æ ‡å‡†\n",
    "            return {\n",
    "                \"max_gelman_rubin\": max_gr,\n",
    "                \"converged\": converged,\n",
    "                \"status\": \"æ”¶æ•›\" if converged else \"å¯èƒ½æœªæ”¶æ•›\",\n",
    "            }\n",
    "        else:\n",
    "            return {\"status\": \"æ— æ³•è®¡ç®—\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mcmc_imputer_on_kpi():\n",
    "    \"\"\"åœ¨çœŸå®KPIæ•°æ®ä¸Šæµ‹è¯•MCMCæ’è¡¥å™¨\"\"\"\n",
    "\n",
    "    # é€‰æ‹©ä¸€ä¸ªæœ‰ç¼ºå¤±å€¼çš„KPIè¿›è¡Œæµ‹è¯•\n",
    "    test_kpi_id = \"da10a69f-d836-3baa-ad40-3e548ecf1fbd\"  # è¿™ä¸ªKPIæœ‰283ä¸ªç¼ºå¤±ç‚¹\n",
    "\n",
    "    print(f\"æµ‹è¯•KPI: {test_kpi_id}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # æå–æµ‹è¯•æ•°æ®\n",
    "    kpi_data = train_ts.filter(pl.col(\"KPI ID\") == test_kpi_id).sort(\"timestamp\")\n",
    "\n",
    "    timestamps = kpi_data[\"timestamp\"].to_numpy()\n",
    "    values = kpi_data[\"value\"].to_numpy()\n",
    "    labels = kpi_data[\"label\"].to_numpy()\n",
    "\n",
    "    print(f\"åŸå§‹æ•°æ®ç‚¹æ•°: {len(values)}\")\n",
    "\n",
    "    # ä½¿ç”¨ä¿®å¤ç‰ˆæœ¬çš„MCMCImputer\n",
    "    print(\"\\nåˆ›å»ºMCMCImputerFixed...\")\n",
    "    imputer = MCMCImputerFixed(\n",
    "        n_iterations=300,  # å‡å°‘è¿­ä»£æ¬¡æ•°ä»¥åŠ å¿«æµ‹è¯•\n",
    "        burn_in=60,\n",
    "        seasonal_period=None,  # è‡ªåŠ¨æ£€æµ‹\n",
    "        use_spectral_residual=True,\n",
    "        adaptive_sampling=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # æ‰§è¡Œæ’è¡¥\n",
    "    print(\"\\nå¼€å§‹MCMCæ’è¡¥...\")\n",
    "    try:\n",
    "        import time\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        complete_timestamps, imputed_values, diagnostics = imputer.fit_transform(\n",
    "            timestamps, values\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        print(f\"\\nâœ… æ’è¡¥å®Œæˆ! è€—æ—¶: {processing_time:.2f}ç§’\")\n",
    "        print(\"è¯Šæ–­ä¿¡æ¯:\")\n",
    "        for key, value in diagnostics.items():\n",
    "            if key != \"sample_std\":  # è·³è¿‡æ ‡å‡†å·®æ•°ç»„çš„æ‰“å°\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "        # è®¡ç®—æ’è¡¥è´¨é‡æŒ‡æ ‡\n",
    "        original_std = np.std(values)\n",
    "        imputed_std = np.std(imputed_values)\n",
    "\n",
    "        # è®¡ç®—ç¼ºå¤±å€¼æ©ç \n",
    "        missing_mask = np.ones(len(complete_timestamps), dtype=bool)\n",
    "        for ts in timestamps:\n",
    "            idx = np.argmin(np.abs(complete_timestamps - ts))\n",
    "            if (\n",
    "                np.abs(complete_timestamps[idx] - ts)\n",
    "                < np.median(np.diff(timestamps)) / 2\n",
    "            ):\n",
    "                missing_mask[idx] = False\n",
    "\n",
    "        n_missing = np.sum(missing_mask)\n",
    "        imputed_missing_std = (\n",
    "            np.std(imputed_values[missing_mask]) if n_missing > 0 else 0\n",
    "        )\n",
    "\n",
    "        print(f\"\\nè´¨é‡è¯„ä¼°:\")\n",
    "        print(f\"  åŸå§‹æ•°æ®ç‚¹æ•°: {len(timestamps)}\")\n",
    "        print(f\"  å®Œæ•´æ•°æ®ç‚¹æ•°: {len(complete_timestamps)}\")\n",
    "        print(f\"  æ’è¡¥ç‚¹æ•°: {n_missing}\")\n",
    "        print(f\"  ç¼ºå¤±ç‡: {n_missing/len(complete_timestamps)*100:.2f}%\")\n",
    "        print(f\"  åŸå§‹æ•°æ®æ ‡å‡†å·®: {original_std:.6f}\")\n",
    "        print(f\"  æ’è¡¥åæ ‡å‡†å·®: {imputed_std:.6f}\")\n",
    "        if n_missing > 0:\n",
    "            print(f\"  æ’è¡¥å€¼æ ‡å‡†å·®: {imputed_missing_std:.6f}\")\n",
    "            print(f\"  æ’è¡¥å€¼æ ‡å‡†å·®æ¯”ç‡: {imputed_missing_std/original_std:.3f}\")\n",
    "\n",
    "        # å¯è§†åŒ–ç»“æœï¼ˆå¦‚æœæ•°æ®ä¸å¤ªå¤§ï¼‰\n",
    "        if len(complete_timestamps) <= 2000:\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            plt.figure(figsize=(15, 10))\n",
    "\n",
    "            # å­å›¾1ï¼šåŸå§‹æ•°æ® vs æ’è¡¥æ•°æ®\n",
    "            plt.subplot(3, 1, 1)\n",
    "\n",
    "            # åŸå§‹è§‚æµ‹ç‚¹\n",
    "            plt.scatter(\n",
    "                timestamps,\n",
    "                values,\n",
    "                c=\"blue\",\n",
    "                s=4,\n",
    "                alpha=0.6,\n",
    "                label=f\"åŸå§‹è§‚æµ‹ ({len(timestamps)}ç‚¹)\",\n",
    "            )\n",
    "\n",
    "            # å®Œæ•´æ—¶é—´åºåˆ—\n",
    "            plt.plot(\n",
    "                complete_timestamps,\n",
    "                imputed_values,\n",
    "                \"g-\",\n",
    "                alpha=0.7,\n",
    "                linewidth=0.8,\n",
    "                label=\"æ’è¡¥ååºåˆ—\",\n",
    "            )\n",
    "\n",
    "            # æ’è¡¥ç‚¹\n",
    "            if n_missing > 0:\n",
    "                plt.scatter(\n",
    "                    complete_timestamps[missing_mask],\n",
    "                    imputed_values[missing_mask],\n",
    "                    c=\"red\",\n",
    "                    s=8,\n",
    "                    alpha=0.8,\n",
    "                    label=f\"æ’è¡¥ç‚¹ ({n_missing}ä¸ª)\",\n",
    "                )\n",
    "\n",
    "            plt.title(f\"MCMCæ’è¡¥ç»“æœ - KPI: {test_kpi_id[:30]}...\")\n",
    "            plt.xlabel(\"æ—¶é—´æˆ³\")\n",
    "            plt.ylabel(\"KPIå€¼\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            # å­å›¾2ï¼šå¼‚å¸¸ç‚¹æ ‡æ³¨\n",
    "            plt.subplot(3, 1, 2)\n",
    "\n",
    "            # åˆ›å»ºå®Œæ•´çš„æ ‡ç­¾æ•°ç»„\n",
    "            complete_labels = np.zeros(len(complete_timestamps))\n",
    "            for i, ts in enumerate(timestamps):\n",
    "                idx = np.argmin(np.abs(complete_timestamps - ts))\n",
    "                if (\n",
    "                    np.abs(complete_timestamps[idx] - ts)\n",
    "                    < np.median(np.diff(timestamps)) / 2\n",
    "                ):\n",
    "                    complete_labels[idx] = labels[i]\n",
    "\n",
    "            normal_mask = complete_labels == 0\n",
    "            anomaly_mask = complete_labels == 1\n",
    "\n",
    "            plt.scatter(\n",
    "                complete_timestamps[normal_mask],\n",
    "                imputed_values[normal_mask],\n",
    "                c=\"green\",\n",
    "                s=4,\n",
    "                alpha=0.6,\n",
    "                label=\"æ­£å¸¸ç‚¹\",\n",
    "            )\n",
    "            if np.any(anomaly_mask):\n",
    "                plt.scatter(\n",
    "                    complete_timestamps[anomaly_mask],\n",
    "                    imputed_values[anomaly_mask],\n",
    "                    c=\"red\",\n",
    "                    s=8,\n",
    "                    alpha=0.8,\n",
    "                    label=f\"å¼‚å¸¸ç‚¹ ({np.sum(anomaly_mask)}ä¸ª)\",\n",
    "                )\n",
    "\n",
    "            plt.title(\"å¼‚å¸¸ç‚¹æ ‡æ³¨\")\n",
    "            plt.xlabel(\"æ—¶é—´æˆ³\")\n",
    "            plt.ylabel(\"KPIå€¼\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            # å­å›¾3ï¼šæ’è¡¥å€¼çš„åˆ†å¸ƒ\n",
    "            plt.subplot(3, 1, 3)\n",
    "            plt.hist(values, bins=40, alpha=0.7, label=\"åŸå§‹å€¼åˆ†å¸ƒ\", density=True)\n",
    "            plt.hist(\n",
    "                imputed_values, bins=40, alpha=0.7, label=\"æ’è¡¥ååˆ†å¸ƒ\", density=True\n",
    "            )\n",
    "            if n_missing > 0:\n",
    "                plt.hist(\n",
    "                    imputed_values[missing_mask],\n",
    "                    bins=20,\n",
    "                    alpha=0.8,\n",
    "                    label=\"æ’è¡¥å€¼åˆ†å¸ƒ\",\n",
    "                    density=True,\n",
    "                )\n",
    "\n",
    "            plt.title(\"å€¼åˆ†å¸ƒæ¯”è¾ƒ\")\n",
    "            plt.xlabel(\"KPIå€¼\")\n",
    "            plt.ylabel(\"å¯†åº¦\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        return {\n",
    "            \"original_data\": (timestamps, values, labels),\n",
    "            \"complete_data\": (complete_timestamps, imputed_values, complete_labels),\n",
    "            \"missing_mask\": missing_mask,\n",
    "            \"diagnostics\": diagnostics,\n",
    "            \"imputer\": imputer,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"quality_metrics\": {\n",
    "                \"original_std\": original_std,\n",
    "                \"imputed_std\": imputed_std,\n",
    "                \"imputed_missing_std\": imputed_missing_std,\n",
    "                \"n_missing\": n_missing,\n",
    "                \"missing_rate\": n_missing / len(complete_timestamps) * 100,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ’è¡¥å¤±è´¥: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# æ‰§è¡Œæµ‹è¯•\n",
    "print(\"ğŸš€ ä½¿ç”¨ä¿®å¤ç‰ˆæœ¬æµ‹è¯•MCMCæ’è¡¥å™¨:\")\n",
    "test_result = test_mcmc_imputer_on_kpi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45665d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMCæ–¹æ³•çš„ä¼˜åŠ¿å’Œæ¯”è¾ƒåˆ†æ\n",
    "def compare_imputation_methods():\n",
    "    \"\"\"æ¯”è¾ƒä¸åŒæ’è¡¥æ–¹æ³•çš„æ•ˆæœ\"\"\"\n",
    "\n",
    "    print(\"ç¼ºå¤±å€¼æ’è¡¥æ–¹æ³•æ¯”è¾ƒ\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # é€‰æ‹©ä¸€ä¸ªæµ‹è¯•KPI\n",
    "    test_kpi_id = \"da10a69f-d836-3baa-ad40-3e548ecf1fbd\"\n",
    "    kpi_data = train_ts.filter(pl.col(\"KPI ID\") == test_kpi_id).sort(\"timestamp\")\n",
    "\n",
    "    timestamps = kpi_data[\"timestamp\"].to_numpy()\n",
    "    values = kpi_data[\"value\"].to_numpy()\n",
    "\n",
    "    # åˆ›å»ºå®Œæ•´æ—¶é—´ç½‘æ ¼\n",
    "    interval = np.median(np.diff(timestamps))\n",
    "    complete_timestamps = np.arange(timestamps[0], timestamps[-1] + interval, interval)\n",
    "    complete_values = np.full(len(complete_timestamps), np.nan)\n",
    "\n",
    "    for i, ts in enumerate(timestamps):\n",
    "        idx = np.argmin(np.abs(complete_timestamps - ts))\n",
    "        if np.abs(complete_timestamps[idx] - ts) < interval / 2:\n",
    "            complete_values[idx] = values[i]\n",
    "\n",
    "    missing_mask = np.isnan(complete_values)\n",
    "    n_missing = np.sum(missing_mask)\n",
    "\n",
    "    print(f\"æµ‹è¯•æ•°æ®: {len(complete_values)} ç‚¹ï¼Œç¼ºå¤± {n_missing} ç‚¹\")\n",
    "\n",
    "    methods_comparison = {}\n",
    "\n",
    "    # 1. çº¿æ€§æ’å€¼ï¼ˆåŸºçº¿æ–¹æ³•ï¼‰\n",
    "    print(\"\\n1. çº¿æ€§æ’å€¼...\")\n",
    "    linear_values = complete_values.copy()\n",
    "    missing_indices = np.where(missing_mask)[0]\n",
    "\n",
    "    for idx in missing_indices:\n",
    "        left_idx = idx - 1\n",
    "        right_idx = idx + 1\n",
    "\n",
    "        while left_idx >= 0 and missing_mask[left_idx]:\n",
    "            left_idx -= 1\n",
    "        while right_idx < len(complete_values) and missing_mask[right_idx]:\n",
    "            right_idx += 1\n",
    "\n",
    "        if left_idx >= 0 and right_idx < len(complete_values):\n",
    "            weight = (idx - left_idx) / (right_idx - left_idx)\n",
    "            linear_values[idx] = (\n",
    "                complete_values[left_idx] * (1 - weight)\n",
    "                + complete_values[right_idx] * weight\n",
    "            )\n",
    "        elif left_idx >= 0:\n",
    "            linear_values[idx] = complete_values[left_idx]\n",
    "        elif right_idx < len(complete_values):\n",
    "            linear_values[idx] = complete_values[right_idx]\n",
    "\n",
    "    methods_comparison[\"çº¿æ€§æ’å€¼\"] = {\n",
    "        \"values\": linear_values,\n",
    "        \"complexity\": \"ä½\",\n",
    "        \"time\": \"å¿«\",\n",
    "        \"seasonal_aware\": False,\n",
    "    }\n",
    "\n",
    "    # 2. å­£èŠ‚æ€§åˆ†è§£æ’å€¼\n",
    "    print(\"2. å­£èŠ‚æ€§åˆ†è§£æ’å€¼...\")\n",
    "    try:\n",
    "        from scipy import signal\n",
    "\n",
    "        # ç®€å•çš„å­£èŠ‚æ€§æ’å€¼\n",
    "        seasonal_values = complete_values.copy()\n",
    "\n",
    "        # æ£€æµ‹å‘¨æœŸ\n",
    "        observed_vals = complete_values[~missing_mask]\n",
    "        if len(observed_vals) > 50:\n",
    "            # ä½¿ç”¨FFTæ£€æµ‹ä¸»è¦å‘¨æœŸ\n",
    "            fft_vals = np.abs(np.fft.fft(observed_vals - np.mean(observed_vals)))\n",
    "            freqs = np.fft.fftfreq(len(observed_vals))\n",
    "            peak_idx = np.argmax(fft_vals[1 : len(fft_vals) // 2]) + 1\n",
    "            if freqs[peak_idx] > 0:\n",
    "                period = int(1 / freqs[peak_idx])\n",
    "                period = max(4, min(period, len(observed_vals) // 4))\n",
    "            else:\n",
    "                period = 24\n",
    "        else:\n",
    "            period = 24\n",
    "\n",
    "        # åŸºäºå‘¨æœŸçš„æ’å€¼\n",
    "        for idx in missing_indices:\n",
    "            seasonal_candidates = []\n",
    "            for offset in [-period, period]:\n",
    "                candidate_idx = idx + offset\n",
    "                if (\n",
    "                    0 <= candidate_idx < len(complete_values)\n",
    "                    and not missing_mask[candidate_idx]\n",
    "                ):\n",
    "                    seasonal_candidates.append(complete_values[candidate_idx])\n",
    "\n",
    "            if seasonal_candidates:\n",
    "                seasonal_values[idx] = np.mean(seasonal_candidates)\n",
    "            else:\n",
    "                # å›é€€åˆ°çº¿æ€§æ’å€¼\n",
    "                seasonal_values[idx] = linear_values[idx]\n",
    "\n",
    "        methods_comparison[\"å­£èŠ‚æ€§æ’å€¼\"] = {\n",
    "            \"values\": seasonal_values,\n",
    "            \"complexity\": \"ä¸­\",\n",
    "            \"time\": \"ä¸­\",\n",
    "            \"seasonal_aware\": True,\n",
    "            \"period\": period,\n",
    "        }\n",
    "    except:\n",
    "        print(\"  å­£èŠ‚æ€§æ’å€¼å¤±è´¥ï¼Œè·³è¿‡\")\n",
    "\n",
    "    # 3. MCMCæ’å€¼ï¼ˆæˆ‘ä»¬çš„æ–¹æ³•ï¼‰\n",
    "    print(\"3. MCMCæ’å€¼...\")\n",
    "    mcmc_imputer = MCMCMissingValueImputer(\n",
    "        n_iterations=200, burn_in=50, seasonal_period=None, random_state=42\n",
    "    )\n",
    "\n",
    "    mcmc_values, mcmc_diagnostics = mcmc_imputer.fit_transform(\n",
    "        complete_timestamps, complete_values, missing_mask\n",
    "    )\n",
    "\n",
    "    methods_comparison[\"MCMCæ’å€¼\"] = {\n",
    "        \"values\": mcmc_values,\n",
    "        \"complexity\": \"é«˜\",\n",
    "        \"time\": \"æ…¢\",\n",
    "        \"seasonal_aware\": True,\n",
    "        \"diagnostics\": mcmc_diagnostics,\n",
    "    }\n",
    "\n",
    "    # 4. è¯„ä¼°æ¯”è¾ƒ\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"æ–¹æ³•æ¯”è¾ƒç»“æœ:\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    observed_values = complete_values[~missing_mask]\n",
    "    observed_std = np.std(observed_values)\n",
    "    observed_mean = np.mean(observed_values)\n",
    "\n",
    "    print(f\"åŸå§‹æ•°æ®ç»Ÿè®¡: å‡å€¼={observed_mean:.6f}, æ ‡å‡†å·®={observed_std:.6f}\")\n",
    "    print()\n",
    "\n",
    "    for method_name, method_data in methods_comparison.items():\n",
    "        imputed_missing = method_data[\"values\"][missing_mask]\n",
    "\n",
    "        # ç»Ÿè®¡æŒ‡æ ‡\n",
    "        imputed_mean = np.mean(imputed_missing)\n",
    "        imputed_std = np.std(imputed_missing)\n",
    "\n",
    "        # ä¸è§‚æµ‹å€¼çš„å·®å¼‚\n",
    "        mean_diff = abs(imputed_mean - observed_mean)\n",
    "        std_ratio = imputed_std / observed_std\n",
    "\n",
    "        print(f\"{method_name}:\")\n",
    "        print(f\"  æ’è¡¥å€¼å‡å€¼: {imputed_mean:.6f} (å·®å¼‚: {mean_diff:.6f})\")\n",
    "        print(f\"  æ’è¡¥å€¼æ ‡å‡†å·®: {imputed_std:.6f} (æ¯”ç‡: {std_ratio:.3f})\")\n",
    "        print(f\"  è®¡ç®—å¤æ‚åº¦: {method_data['complexity']}\")\n",
    "        print(f\"  è®¡ç®—é€Ÿåº¦: {method_data['time']}\")\n",
    "        print(f\"  å­£èŠ‚æ€§æ„ŸçŸ¥: {method_data['seasonal_aware']}\")\n",
    "\n",
    "        if \"period\" in method_data:\n",
    "            print(f\"  æ£€æµ‹å‘¨æœŸ: {method_data['period']}\")\n",
    "        if \"diagnostics\" in method_data:\n",
    "            conv_status = (\n",
    "                method_data[\"diagnostics\"]\n",
    "                .get(\"convergence_diagnostic\", {})\n",
    "                .get(\"status\", \"æœªçŸ¥\")\n",
    "            )\n",
    "            print(f\"  æ”¶æ•›çŠ¶æ€: {conv_status}\")\n",
    "        print()\n",
    "\n",
    "    return methods_comparison\n",
    "\n",
    "\n",
    "# æ‰§è¡Œæ¯”è¾ƒ\n",
    "print(\"å¼€å§‹æ’è¡¥æ–¹æ³•æ¯”è¾ƒ...\")\n",
    "comparison_results = compare_imputation_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€æºå®ç°å‚è€ƒæ€»ç»“å’Œæœ€ä½³å®è·µå»ºè®®\n",
    "print(\"å¼€æºå®ç°å‚è€ƒæ€»ç»“\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "ğŸ“š ä¸»è¦å‚è€ƒçš„å¼€æºå®ç°:\n",
    "\n",
    "1. ğŸ”¥ Donut (NetManAIOps/donut) - VAEè®ºæ–‡å®˜æ–¹å®ç°\n",
    "   - GitHub: https://github.com/NetManAIOps/donut\n",
    "   - ç‰¹ç‚¹: \n",
    "     * å®Œæ•´çš„VAEå¼‚å¸¸æ£€æµ‹æµç¨‹\n",
    "     * é«˜è´¨é‡çš„æ—¶é—´åºåˆ—é¢„å¤„ç†\n",
    "     * MCMCç¼ºå¤±å€¼æ’è¡¥\n",
    "     * å·¥ä¸šçº§ä»£ç è´¨é‡\n",
    "   - æ ¸å¿ƒæ–‡ä»¶:\n",
    "     * donut/preprocessing.py - æ—¶é—´æˆ³å®Œå–„å’Œæ ‡å‡†åŒ–\n",
    "     * donut/model.py - VAEæ¨¡å‹å®ç°\n",
    "     * æ”¯æŒiterative_masked_reconstructå‡½æ•°\n",
    "\n",
    "2. ğŸš€ Self-adversarial VAE (YeongHyeon/adVAE)\n",
    "   - GitHub: https://github.com/YeongHyeon/adVAE  \n",
    "   - ç‰¹ç‚¹:\n",
    "     * å¯¹æŠ—è®­ç»ƒæœºåˆ¶\n",
    "     * è°±æ®‹å·®å¼‚å¸¸æ£€æµ‹\n",
    "     * è‡ªé€‚åº”å­¦ä¹ ç­–ç•¥\n",
    "     * å¤„ç†å¼‚å¸¸æ±¡æŸ“é—®é¢˜\n",
    "\n",
    "3. ğŸ“Š Alibi-detect (SeldonIO/alibi-detect)\n",
    "   - GitHub: https://github.com/SeldonIO/alibi-detect\n",
    "   - ç‰¹ç‚¹:\n",
    "     * åŒ…å«è°±æ®‹å·®æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹\n",
    "     * å¤šç§VAEå˜ä½“å®ç°\n",
    "     * ç”Ÿäº§çº§å¼‚å¸¸æ£€æµ‹åº“\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ”§ æˆ‘ä»¬å®ç°çš„æ”¹è¿›:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "improvements = [\n",
    "    \"âœ… åŸºäºDonutçš„complete_timestampæ–¹æ³• - æ›´å‡†ç¡®çš„ç¼ºå¤±å€¼æ£€æµ‹\",\n",
    "    \"âœ… é«˜çº§å­£èŠ‚æ€§æ£€æµ‹ - FFT + è‡ªç›¸å…³ + æ—¶é—´é—´éš”æ¨ç†\",\n",
    "    \"âœ… è°±æ®‹å·®å¼‚å¸¸æ£€æµ‹ - è¯†åˆ«å¼‚å¸¸æ¨¡å¼ï¼Œè°ƒæ•´é‡‡æ ·ç­–ç•¥\",\n",
    "    \"âœ… è‡ªé€‚åº”Gibbsé‡‡æ · - æ ¹æ®å¼‚å¸¸åˆ†æ•°å’Œè¿­ä»£è¿›åº¦è°ƒæ•´\",\n",
    "    \"âœ… æ›´é²æ£’çš„è¶‹åŠ¿ä¼°è®¡ - ä½¿ç”¨ä¸­ä½æ•°å’Œå¹³æ»‘æ»¤æ³¢\",\n",
    "    \"âœ… æ”¶æ•›è¯Šæ–­ - Gelman-Rubinç»Ÿè®¡é‡\",\n",
    "    \"âœ… å‚æ•°è‡ªé€‚åº” - æ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨è°ƒæ•´\",\n",
    "]\n",
    "\n",
    "for improvement in improvements:\n",
    "    print(f\"  {improvement}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ æœ€ä½³å®è·µå»ºè®®:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_practices = [\n",
    "    \"1. æ•°æ®é¢„å¤„ç†:\",\n",
    "    \"   - ä½¿ç”¨complete_timestampç¡®ä¿æ—¶é—´åºåˆ—å®Œæ•´æ€§\",\n",
    "    \"   - æ ‡å‡†åŒ–KPIå€¼åˆ°åˆé€‚èŒƒå›´\",\n",
    "    \"   - æ£€æµ‹å¹¶å¤„ç†æ˜æ˜¾çš„å¼‚å¸¸å€¼\",\n",
    "    \"\",\n",
    "    \"2. å­£èŠ‚æ€§å¤„ç†:\",\n",
    "    \"   - ä¼˜å…ˆä½¿ç”¨ä¸šåŠ¡çŸ¥è¯†ç¡®å®šå‘¨æœŸï¼ˆå¦‚24å°æ—¶ã€7å¤©ï¼‰\",\n",
    "    \"   - ç»“åˆFFTå’Œè‡ªç›¸å…³éªŒè¯æ£€æµ‹ç»“æœ\",\n",
    "    \"   - è€ƒè™‘å¤šé‡å­£èŠ‚æ€§ï¼ˆå°æ—¶+å¤©+å‘¨ï¼‰\",\n",
    "    \"\",\n",
    "    \"3. MCMCå‚æ•°è°ƒä¼˜:\",\n",
    "    \"   - è¿­ä»£æ¬¡æ•°: 500-1000ï¼ˆå–å†³äºæ•°æ®å¤æ‚åº¦ï¼‰\",\n",
    "    \"   - Burn-in: æ€»è¿­ä»£çš„20-30%\",\n",
    "    \"   - å¯ç”¨è‡ªé€‚åº”é‡‡æ ·å’Œè°±æ®‹å·®æ£€æµ‹\",\n",
    "    \"\",\n",
    "    \"4. è´¨é‡è¯„ä¼°:\",\n",
    "    \"   - æ£€æŸ¥æ’è¡¥å€¼çš„ç»Ÿè®¡åˆ†å¸ƒ\",\n",
    "    \"   - éªŒè¯å­£èŠ‚æ€§æ¨¡å¼ä¿æŒ\",\n",
    "    \"   - ç›‘æ§æ”¶æ•›è¯Šæ–­æŒ‡æ ‡\",\n",
    "    \"\",\n",
    "    \"5. ç”Ÿäº§éƒ¨ç½²:\",\n",
    "    \"   - æ‰¹é‡å¤„ç†æ—¶ä½¿ç”¨è¾ƒå°‘è¿­ä»£æ•°ï¼ˆ200-300ï¼‰\",\n",
    "    \"   - å¯¹é«˜ç¼ºå¤±ç‡æ•°æ®ï¼ˆ>50%ï¼‰ä½¿ç”¨ç®€å•æ–¹æ³•\",\n",
    "    \"   - ä¿å­˜æ’è¡¥è¯Šæ–­ä¿¡æ¯ç”¨äºè´¨é‡ç›‘æ§\",\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"  {practice}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "performance_comparison = \"\"\"\n",
    "æ–¹æ³•æ¯”è¾ƒ (åŸºäºæˆ‘ä»¬çš„æµ‹è¯•):\n",
    "\n",
    "| æ–¹æ³•           | å‡†ç¡®æ€§ | é€Ÿåº¦ | å­£èŠ‚æ€§ | å¼‚å¸¸å¤„ç† | é€‚ç”¨åœºæ™¯ |\n",
    "|---------------|--------|------|--------|----------|----------|\n",
    "| çº¿æ€§æ’å€¼       | â­â­    | â­â­â­â­â­ | âŒ     | âŒ       | ç®€å•åœºæ™¯ |\n",
    "| å­£èŠ‚æ€§æ’å€¼     | â­â­â­   | â­â­â­â­  | â­â­â­   | âŒ       | è§„å¾‹æ•°æ® |\n",
    "| åŸç‰ˆMCMC      | â­â­â­â­  | â­â­    | â­â­â­   | â­â­      | ä¸€èˆ¬KPI  |\n",
    "| æ”¹è¿›MCMC      | â­â­â­â­â­ | â­â­    | â­â­â­â­â­ | â­â­â­â­â­   | å¤æ‚KPI  |\n",
    "\n",
    "æ¨èç­–ç•¥:\n",
    "- å¿«é€ŸåŸå‹: çº¿æ€§æ’å€¼\n",
    "- ç”Ÿäº§ç¯å¢ƒ: æ”¹è¿›MCMC (ç¼ºå¤±ç‡<30%) + å­£èŠ‚æ€§æ’å€¼ (ç¼ºå¤±ç‡30-50%)\n",
    "- å…³é”®KPI: æ”¹è¿›MCMC + äººå·¥éªŒè¯\n",
    "\"\"\"\n",
    "\n",
    "print(performance_comparison)\n",
    "\n",
    "print(f\"\\nğŸ”— ç›¸å…³èµ„æº:\")\n",
    "print(\"-\" * 50)\n",
    "resources = [\n",
    "    \"ğŸ“„ è®ºæ–‡: Unsupervised Anomaly Detection via VAE for Seasonal KPIs (WWW 2018)\",\n",
    "    \"ğŸ“„ è®ºæ–‡: Self-adversarial VAE with Spectral Residual (Neurocomputing 2021)\",\n",
    "    \"ğŸ’» Donutå®˜æ–¹å®ç°: https://github.com/NetManAIOps/donut\",\n",
    "    \"ğŸ’» Self-adversarial VAE: https://github.com/YeongHyeon/adVAE\",\n",
    "    \"ğŸ’» Alibi-detectåº“: https://github.com/SeldonIO/alibi-detect\",\n",
    "    \"ğŸ“š æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ç»¼è¿°: https://github.com/lzz19980125/awesome-multivariate-time-series-anomaly-detection-algorithms\",\n",
    "]\n",
    "\n",
    "for resource in resources:\n",
    "    print(f\"  {resource}\")\n",
    "\n",
    "print(f\"\\nâœ¨ æ€»ç»“:\")\n",
    "print(\"-\" * 50)\n",
    "print(\n",
    "    \"\"\"\n",
    "æˆ‘ä»¬çš„å®ç°ç»“åˆäº†ä¸¤ç¯‡é‡è¦è®ºæ–‡çš„ä¼˜ç‚¹:\n",
    "1. Donutçš„å·¥ä¸šçº§é¢„å¤„ç†å’ŒMCMCæ’è¡¥\n",
    "2. Self-adversarial VAEçš„è°±æ®‹å·®å¼‚å¸¸æ£€æµ‹\n",
    "\n",
    "è¿™ç§ç»„åˆæ–¹æ³•ç‰¹åˆ«é€‚åˆå¤„ç†:\n",
    "- å…·æœ‰å¤æ‚å­£èŠ‚æ€§çš„KPIæ•°æ®\n",
    "- å­˜åœ¨å¼‚å¸¸æ±¡æŸ“çš„è®­ç»ƒæ•°æ®  \n",
    "- éœ€è¦é«˜è´¨é‡æ’è¡¥çš„å…³é”®ä¸šåŠ¡æŒ‡æ ‡\n",
    "\n",
    "å»ºè®®åœ¨å®é™…ä½¿ç”¨ä¸­æ ¹æ®æ•°æ®ç‰¹å¾å’Œæ€§èƒ½è¦æ±‚é€‰æ‹©åˆé€‚çš„æ–¹æ³•ã€‚\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ea9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_points(df, kpi_id):\n",
    "    \"\"\"Analyze missing points in a time series\"\"\"\n",
    "    kpi_data = df[df[\"KPI ID\"] == kpi_id].copy().sort_values(\"timestamp\")\n",
    "\n",
    "    if len(kpi_data) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    # Calculate expected time intervals\n",
    "    timestamps = kpi_data[\"timestamp\"].values\n",
    "    time_diffs = np.diff(timestamps)\n",
    "\n",
    "    # Most common interval (should be 60 seconds for minute-level data)\n",
    "    interval = np.median(time_diffs)\n",
    "\n",
    "    # Expected full range\n",
    "    start_time = timestamps[0]\n",
    "    end_time = timestamps[-1]\n",
    "    expected_length = int((end_time - start_time) / interval) + 1\n",
    "\n",
    "    # Find missing timestamps\n",
    "    expected_timestamps = np.arange(start_time, end_time + interval, interval)\n",
    "    missing_timestamps = set(expected_timestamps) - set(timestamps)\n",
    "\n",
    "    return kpi_data, missing_timestamps, interval\n",
    "\n",
    "\n",
    "def interpolate_missing_points(kpi_data, missing_timestamps, interval):\n",
    "    \"\"\"Perform linear interpolation for missing points\"\"\"\n",
    "    if len(missing_timestamps) == 0:\n",
    "        return kpi_data\n",
    "\n",
    "    # Create complete timestamp range\n",
    "    start_time = kpi_data[\"timestamp\"].min()\n",
    "    end_time = kpi_data[\"timestamp\"].max()\n",
    "    complete_timestamps = np.arange(start_time, end_time + interval, interval)\n",
    "\n",
    "    # Create complete dataframe with missing points\n",
    "    complete_df = pd.DataFrame(\n",
    "        {\"timestamp\": complete_timestamps, \"KPI ID\": kpi_data[\"KPI ID\"].iloc[0]}\n",
    "    )\n",
    "\n",
    "    # Merge with original data\n",
    "    merged_df = complete_df.merge(kpi_data, on=[\"timestamp\", \"KPI ID\"], how=\"left\")\n",
    "\n",
    "    # Interpolate missing values\n",
    "    merged_df[\"value\"] = merged_df[\"value\"].interpolate(method=\"linear\")\n",
    "    merged_df[\"label\"] = (\n",
    "        merged_df[\"label\"].fillna(0).astype(int)\n",
    "    )  # Assume missing labels are normal (0)\n",
    "\n",
    "    # Maintain original column order: timestamp, value, label, KPI ID\n",
    "    merged_df = merged_df[[\"timestamp\", \"value\", \"label\", \"KPI ID\"]]\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def ensure_multiple_of_32(df):\n",
    "    \"\"\"Ensure sequence length is a multiple of 32\"\"\"\n",
    "    current_length = len(df)\n",
    "    target_length = (current_length // 32) * 32\n",
    "\n",
    "    if target_length == 0:\n",
    "        return (\n",
    "            df.iloc[:32] if current_length >= 32 else df\n",
    "        )  # Keep at least 32 points if possible\n",
    "\n",
    "    return df.iloc[:target_length]\n",
    "\n",
    "\n",
    "def process_single_kpi(df, kpi_id, verbose=False):\n",
    "    \"\"\"Process a single KPI time series\"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\nå¤„ç† KPI: {kpi_id}\")\n",
    "\n",
    "    # Step 1: Analyze missing points\n",
    "    kpi_data, missing_timestamps, interval = analyze_missing_points(df, kpi_id)\n",
    "\n",
    "    if kpi_data is None:\n",
    "        if verbose:\n",
    "            print(f\"  é”™è¯¯: KPI {kpi_id} æ— æ•°æ®\")\n",
    "        return None\n",
    "\n",
    "    original_length = len(kpi_data)\n",
    "    missing_count = len(missing_timestamps) if missing_timestamps else 0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  åŸå§‹é•¿åº¦: {original_length}\")\n",
    "        print(f\"  ç¼ºå¤±ç‚¹æ•°: {missing_count}\")\n",
    "        print(f\"  æ—¶é—´é—´éš”: {interval} ç§’\")\n",
    "\n",
    "    # Step 2: Interpolate missing points\n",
    "    if missing_count > 0:\n",
    "        kpi_data = interpolate_missing_points(kpi_data, missing_timestamps, interval)\n",
    "        if verbose:\n",
    "            print(f\"  æ’å€¼åé•¿åº¦: {len(kpi_data)}\")\n",
    "\n",
    "    # Step 3: Ensure multiple of 32\n",
    "    kpi_data = ensure_multiple_of_32(kpi_data)\n",
    "    final_length = len(kpi_data)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  æœ€ç»ˆé•¿åº¦: {final_length} (32çš„å€æ•°: {final_length % 32 == 0})\")\n",
    "        removed_points = (\n",
    "            (len(kpi_data) if missing_count == 0 else len(kpi_data) + missing_count)\n",
    "            + original_length\n",
    "            - final_length\n",
    "        )\n",
    "        print(f\"  ç§»é™¤çš„ç‚¹æ•°: {max(0, original_length + missing_count - final_length)}\")\n",
    "\n",
    "    return kpi_data\n",
    "\n",
    "\n",
    "def process_train_data(input_file, output_file=None):\n",
    "    \"\"\"Process training data - simplified version for train data\"\"\"\n",
    "\n",
    "    print(\"è®­ç»ƒæ•°æ®å¤„ç†\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"è¾“å…¥æ–‡ä»¶: {input_file}\")\n",
    "\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {input_file}\")\n",
    "        return None\n",
    "\n",
    "    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å\n",
    "    if input_file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(input_file)\n",
    "    elif input_file.endswith(\".hdf\"):\n",
    "        try:\n",
    "            df = pd.read_hdf(input_file)\n",
    "        except Exception as e:\n",
    "            print(f\"è¯»å–HDFæ–‡ä»¶å¤±è´¥: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_file}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"åŸå§‹æ•°æ®: {len(df):,} è¡Œ, {df.shape[1]} åˆ—\")\n",
    "\n",
    "    # æ£€æŸ¥æ•°æ®æ ¼å¼\n",
    "    required_columns = [\"timestamp\", \"value\", \"label\", \"KPI ID\"]\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\n",
    "            f\"é”™è¯¯: æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—ã€‚éœ€è¦: {required_columns}, å®é™…: {df.columns.tolist()}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Get unique KPIs\n",
    "    kpi_ids = df[\"KPI ID\"].unique()\n",
    "    print(f\"KPIæ•°é‡: {len(kpi_ids)}\")\n",
    "\n",
    "    # Process each KPI\n",
    "    processed_data = []\n",
    "    processing_stats = []\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"å¤„ç†å„ä¸ªKPI...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i, kpi_id in enumerate(kpi_ids, 1):\n",
    "        print(f\"\\n[{i}/{len(kpi_ids)}] å¤„ç† KPI: {str(kpi_id)[:50]}...\")\n",
    "\n",
    "        # Get original stats\n",
    "        original_data = df[df[\"KPI ID\"] == kpi_id]\n",
    "        original_length = len(original_data)\n",
    "        original_anomalies = original_data[\"label\"].sum()\n",
    "\n",
    "        # Process the KPI\n",
    "        processed_kpi = process_single_kpi(df, kpi_id, verbose=True)\n",
    "\n",
    "        if processed_kpi is not None:\n",
    "            processed_data.append(processed_kpi)\n",
    "\n",
    "            # Calculate stats\n",
    "            final_length = len(processed_kpi)\n",
    "            final_anomalies = processed_kpi[\"label\"].sum()\n",
    "            missing_filled = (\n",
    "                final_length - original_length if final_length >= original_length else 0\n",
    "            )\n",
    "            points_removed = max(0, original_length - final_length)\n",
    "\n",
    "            processing_stats.append(\n",
    "                {\n",
    "                    \"KPI_ID\": kpi_id,\n",
    "                    \"original_length\": original_length,\n",
    "                    \"final_length\": final_length,\n",
    "                    \"missing_filled\": missing_filled,\n",
    "                    \"points_removed\": points_removed,\n",
    "                    \"original_anomalies\": original_anomalies,\n",
    "                    \"final_anomalies\": final_anomalies,\n",
    "                    \"is_multiple_32\": final_length % 32 == 0,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  è·³è¿‡ KPI {kpi_id} (å¤„ç†å¤±è´¥)\")\n",
    "\n",
    "    # Combine all processed data\n",
    "    if processed_data:\n",
    "        final_df = pd.concat(processed_data, ignore_index=True)\n",
    "        # Ensure correct column order: timestamp, value, label, KPI ID\n",
    "        final_df = final_df[[\"timestamp\", \"value\", \"label\", \"KPI ID\"]]\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"å¤„ç†ç»“æœæ±‡æ€»\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Overall statistics\n",
    "        total_original = len(df)\n",
    "        total_final = len(final_df)\n",
    "        total_missing_filled = sum(stat[\"missing_filled\"] for stat in processing_stats)\n",
    "        total_removed = sum(stat[\"points_removed\"] for stat in processing_stats)\n",
    "\n",
    "        print(f\"æ€»ä½“ç»Ÿè®¡:\")\n",
    "        print(f\"  åŸå§‹æ•°æ®ç‚¹: {total_original:,}\")\n",
    "        print(f\"  æœ€ç»ˆæ•°æ®ç‚¹: {total_final:,}\")\n",
    "        print(f\"  å¡«è¡¥ç¼ºå¤±ç‚¹: {total_missing_filled:,}\")\n",
    "        print(f\"  ç§»é™¤æ•°æ®ç‚¹: {total_removed:,}\")\n",
    "        print(f\"  å‡€å˜åŒ–: {total_final - total_original:+,}\")\n",
    "\n",
    "        # Per-KPI statistics\n",
    "        stats_df = pd.DataFrame(processing_stats)\n",
    "\n",
    "        print(f\"\\nå„KPIå¤„ç†ç»Ÿè®¡:\")\n",
    "        print(f\"  æˆåŠŸå¤„ç†çš„KPI: {len(stats_df)}/{len(kpi_ids)}\")\n",
    "        print(f\"  æ‰€æœ‰åºåˆ—éƒ½æ˜¯32çš„å€æ•°: {stats_df['is_multiple_32'].all()}\")\n",
    "        print(f\"  å¹³å‡åºåˆ—é•¿åº¦: {stats_df['final_length'].mean():.0f}\")\n",
    "        print(f\"  æœ€çŸ­åºåˆ—: {stats_df['final_length'].min():,}\")\n",
    "        print(f\"  æœ€é•¿åºåˆ—: {stats_df['final_length'].max():,}\")\n",
    "\n",
    "        # Save processed data\n",
    "        if output_file:\n",
    "            print(f\"\\nä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°: {output_file}\")\n",
    "            final_df.to_csv(output_file, index=False)\n",
    "            print(f\"ä¿å­˜å®Œæˆ: {len(final_df):,} è¡Œ\")\n",
    "\n",
    "            # Save processing statistics\n",
    "            stats_file = output_file.replace(\".csv\", \"_processing_stats.csv\")\n",
    "            stats_df.to_csv(stats_file, index=False)\n",
    "            print(f\"å¤„ç†ç»Ÿè®¡ä¿å­˜åˆ°: {stats_file}\")\n",
    "\n",
    "        return final_df, stats_df\n",
    "\n",
    "    else:\n",
    "        print(\"é”™è¯¯: æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•KPI\")\n",
    "        return None, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectrum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
